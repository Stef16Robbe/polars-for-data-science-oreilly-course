{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1653908d",
   "metadata": {},
   "source": [
    "# 10. Integrating Polars Into the Data Science Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a37bab",
   "metadata": {},
   "source": [
    "By this point in this course, we've gained a lot of dexterity with `polars`; now, it's time to use it in a real data science problem!\n",
    "\n",
    "The problem we will be trying to solve is **tip amount prediction**, using the NYC taxi rides data that we've been using throughout this course. The goal here is to provide a service to taxi drivers which, before starting a taxi ride, they can plug the details of their ride into, and receive a prediction for what tip amount that customer can be expected to pay. They could then use this information to inform themselves on how to best interact with their passengers to increase their tip; furthermore, feature importance insights from such a model could help a taxi driver decide the best places in the city to work, or the best times in the week, etc.\n",
    "\n",
    "We'll go through the entire data science process:\n",
    "1. Setting Up the Environment\n",
    "2. Loading Data\n",
    "4. Brief Data Exploration\n",
    "3. Cleaning and Preparing the Data\n",
    "4. Feature Engineering and Selection\n",
    "5. Model Building\n",
    "6. Model Evaluation\n",
    "7. Discussion\n",
    "\n",
    "Let's get into it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a90b5b",
   "metadata": {},
   "source": [
    "## 10.1. Setting up the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6591970e",
   "metadata": {},
   "source": [
    "Import `polars` as usual..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f2da0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa7dbbc",
   "metadata": {},
   "source": [
    "But also import some other tools from the data science suite that we'll need throughout this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21f37fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b91227",
   "metadata": {},
   "source": [
    "## 10.2. Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465fb7bf",
   "metadata": {},
   "source": [
    "First, we load the data about zones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fc8c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "zone_column_rename_mapping = {\n",
    "    \"LocationID\": \"location_id\",\n",
    "    \"Borough\": \"borough\",\n",
    "    \"Zone\": \"zone\",\n",
    "}\n",
    "zones_df = (\n",
    "    pl.read_parquet(\"../data/taxi_zone_lookup.parquet\")\n",
    "    .rename(zone_column_rename_mapping)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92783d0b",
   "metadata": {},
   "source": [
    "Then, we load in the yellow trips taxi data, for both february and march (using `*` file pattern notation), joining in the zone information as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740af260",
   "metadata": {},
   "outputs": [],
   "source": [
    "yellow_rides_column_rename_mapping = {\n",
    "    \"VendorID\": \"vendor_id\",\n",
    "    \"RatecodeID\": \"ratecode_id\",\n",
    "    \"PULocationID\": \"pu_location_id\",\n",
    "    \"DOLocationID\": \"do_location_id\",\n",
    "    \"Airport_fee\": \"airport_fee\",\n",
    "}\n",
    "\n",
    "zone_df_columns = [\"borough\", \"zone\", \"service_zone\",]\n",
    "\n",
    "rides_df_raw = (\n",
    "    pl.read_parquet(\"../data/yellow_tripdata_2024-*.parquet\")\n",
    "    .rename(yellow_rides_column_rename_mapping)\n",
    "    .join(zones_df, left_on=\"pu_location_id\", right_on=\"location_id\")\n",
    "    .rename({zone_df_column: f\"pu_{zone_df_column}\" for zone_df_column in zone_df_columns})\n",
    "    .join(zones_df, left_on=\"do_location_id\", right_on=\"location_id\")\n",
    "    .rename({zone_df_column: f\"do_{zone_df_column}\" for zone_df_column in zone_df_columns})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594efbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(rides_df_raw.shape)\n",
    "display(rides_df_raw.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1176c64",
   "metadata": {},
   "source": [
    "Looks good to go!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839ac396",
   "metadata": {},
   "source": [
    "## 10.3. Brief Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec14c8b9",
   "metadata": {},
   "source": [
    "Before we proceed with any machine learning, let's first have a look at the data, viewed through the lens of the machine learning problem we're going to solve, keeping an eye out for anything we'll need to take care of during data cleaning and preparation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e543210f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rides_df_raw.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec815bb3",
   "metadata": {},
   "source": [
    "A few notes:\n",
    "- Some rides had pickup and dropoff times that took place outside of the data's known time range of beginning of february 2024 until the end of march 2024 (for example the minimum `\"tpep_pickup_datetime\"` is `\"2002-12-31 22:17:10\"`.\n",
    "- A few columns are missing data, all in exactly `611800` rows (about `10%` of the data): `\"passenger_count\"`, `\"ratecode_id\"`, `\"store_and_fwd_flag\"`, `\"congestion_surcharge\"`, and `\"airport_fee\"`.\n",
    "- As we've seen throughout the course, some fields have impossibly subzero values, namely `\"trip_distance\"` and some of the currency columns (e.g. the mnimum `\"fare_amount\"` was `-$999`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823fed1f",
   "metadata": {},
   "source": [
    "To the point that a few columns are missing data, let's double check the percentage of that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a5da86",
   "metadata": {},
   "outputs": [],
   "source": [
    "rides_df_raw.select(pl.all().null_count() / pl.len())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bc631c",
   "metadata": {},
   "source": [
    "Since they are all the exact same number of missing rows, hopefully they are null together. Let's check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637613a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_with_null = [\"passenger_count\", \"ratecode_id\", \"store_and_fwd_flag\", \"congestion_surcharge\", \"airport_fee\"]\n",
    "rides_df_raw.select(\n",
    "    pl.all_horizontal(pl.col(cols_with_null).is_null()).sum() / pl.len()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9db238",
   "metadata": {},
   "source": [
    "Indeed, they are all null together! We'll take care of this when it comes time to clean and prepare the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbea1b1",
   "metadata": {},
   "source": [
    "Next, let's plot an ECDF of the target variable. We can do this by passing a polars series directly to matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01a6573",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = rides_df_raw[\"tip_amount\"]\n",
    "plt.ecdf(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc08caa",
   "metadata": {},
   "source": [
    "Already we see some weird stuff with `\"tip_amount\"`--the highest tip "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c9d35f",
   "metadata": {},
   "source": [
    "## 10.3. Cleaning and Preparing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33617e5d",
   "metadata": {},
   "source": [
    "Over the course of this course, we've compiled some filters of impossible data, such as rides without passengers, and rides that had a pickup that occurred after the dropoff, and some of the other data issues we just discovered in the prior section (such as missing values). We'd like to handle this bad data before jumping into machine learning, so first let's quantify the scope of the issue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16111a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_impossible_data_filter = (\n",
    "    pl.col(\"tpep_pickup_datetime\").lt(pl.col(\"tpep_dropoff_datetime\")) &\n",
    "    pl.col(\"passenger_count\").gt(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce99538b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rides_df_raw.select(remove_impossible_data_filter.mean().alias(\"fraction_data_usable\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e44cb2",
   "metadata": {},
   "source": [
    "Looks fine to remove!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4399a8f",
   "metadata": {},
   "source": [
    "While we're here, before we go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86145fa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee581389",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dab6b518",
   "metadata": {},
   "source": [
    " However, we don't want to remove all the bad data just yet--**we only want to remove bad data from our train data**. This is because, when the model is performing live in the wild, we won't necessarily be guaranteed that the data will be clean in the way we'd like it to be; when taxi drivers use our system, perhaps they accidentally enter \"0 passengers\" "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
