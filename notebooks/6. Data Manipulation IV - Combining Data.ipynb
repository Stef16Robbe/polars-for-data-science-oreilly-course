{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1653908d",
   "metadata": {},
   "source": [
    "# 6. Data Manipulation IV - Combining Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a37bab",
   "metadata": {},
   "source": [
    "The goal of this module is to learn to perform Polars queries that involve combining data. More specifically, we'll cover two new Query Statements:\n",
    "1. Joining dataframes with `.join()`.\n",
    "2. Concatenating dataframes with `.concat()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a90b5b",
   "metadata": {},
   "source": [
    "But first we import `polars`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f2da0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b346457f",
   "metadata": {},
   "source": [
    "... and load the data, this time changing the name from `df` to `march_yellow_rides_df` for what's in store in this module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b85b1837",
   "metadata": {},
   "outputs": [],
   "source": [
    "yellow_rides_column_rename_mapping = {\n",
    "    \"VendorID\": \"vendor_id\",\n",
    "    \"RatecodeID\": \"ratecode_id\",\n",
    "    \"PULocationID\": \"pu_location_id\",\n",
    "    \"DOLocationID\": \"do_location_id\",\n",
    "    \"Airport_fee\": \"airport_fee\",\n",
    "}\n",
    "march_yellow_rides_df = (\n",
    "    pl.read_parquet(\"../data/yellow_tripdata_2024-03.parquet\")\n",
    "    .rename(yellow_rides_column_rename_mapping)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1109c3fe",
   "metadata": {},
   "source": [
    "## 5.1. Joining Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd114a72",
   "metadata": {},
   "source": [
    "In the previous module, we were interested in seeing some summary statistics about each of the different pickup location IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12bff3c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>pu_location_id</th><th>count_trips</th><th>trip_distance_min</th><th>trip_distance_max</th><th>trip_distance_mean</th></tr><tr><td>i32</td><td>u32</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>161</td><td>163269</td><td>0.0</td><td>51066.77</td><td>2.692728</td></tr><tr><td>132</td><td>157706</td><td>0.0</td><td>9211.95</td><td>15.76677</td></tr><tr><td>237</td><td>155631</td><td>0.0</td><td>44866.77</td><td>2.096025</td></tr><tr><td>236</td><td>146044</td><td>0.0</td><td>109619.96</td><td>3.481146</td></tr><tr><td>162</td><td>123805</td><td>0.0</td><td>57408.32</td><td>3.030781</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 5)\n",
       "┌────────────────┬─────────────┬───────────────────┬───────────────────┬────────────────────┐\n",
       "│ pu_location_id ┆ count_trips ┆ trip_distance_min ┆ trip_distance_max ┆ trip_distance_mean │\n",
       "│ ---            ┆ ---         ┆ ---               ┆ ---               ┆ ---                │\n",
       "│ i32            ┆ u32         ┆ f64               ┆ f64               ┆ f64                │\n",
       "╞════════════════╪═════════════╪═══════════════════╪═══════════════════╪════════════════════╡\n",
       "│ 161            ┆ 163269      ┆ 0.0               ┆ 51066.77          ┆ 2.692728           │\n",
       "│ 132            ┆ 157706      ┆ 0.0               ┆ 9211.95           ┆ 15.76677           │\n",
       "│ 237            ┆ 155631      ┆ 0.0               ┆ 44866.77          ┆ 2.096025           │\n",
       "│ 236            ┆ 146044      ┆ 0.0               ┆ 109619.96         ┆ 3.481146           │\n",
       "│ 162            ┆ 123805      ┆ 0.0               ┆ 57408.32          ┆ 3.030781           │\n",
       "└────────────────┴─────────────┴───────────────────┴───────────────────┴────────────────────┘"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "location_id_summary_df = (\n",
    "    march_yellow_rides_df\n",
    "    .group_by(\"pu_location_id\")\n",
    "    .agg(\n",
    "        pl.len().alias(\"count_trips\"),\n",
    "        pl.col(\"trip_distance\").min().name.suffix(\"_min\"),\n",
    "        pl.col(\"trip_distance\").max().name.suffix(\"_max\"),\n",
    "        pl.col(\"trip_distance\").mean().name.suffix(\"_mean\"),\n",
    "    )\n",
    "    .sort(pl.col(\"count_trips\"), descending=True)\n",
    ")\n",
    "location_id_summary_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcd2b73",
   "metadata": {},
   "source": [
    "This is nice, but if we try to present these results to any sort of business stakeholder, the first thing they'll say is \"What are these location IDs? What is their actual name?\" Thankfully, that information is stored in another file, `\"taxi_zone_lookup.parquet\"`. Let's load it in..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5af5a206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>LocationID</th><th>Borough</th><th>Zone</th><th>service_zone</th></tr><tr><td>i32</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>1</td><td>&quot;EWR&quot;</td><td>&quot;Newark Airport&quot;</td><td>&quot;EWR&quot;</td></tr><tr><td>2</td><td>&quot;Queens&quot;</td><td>&quot;Jamaica Bay&quot;</td><td>&quot;Boro Zone&quot;</td></tr><tr><td>3</td><td>&quot;Bronx&quot;</td><td>&quot;Allerton/Pelham Gardens&quot;</td><td>&quot;Boro Zone&quot;</td></tr><tr><td>4</td><td>&quot;Manhattan&quot;</td><td>&quot;Alphabet City&quot;</td><td>&quot;Yellow Zone&quot;</td></tr><tr><td>5</td><td>&quot;Staten Island&quot;</td><td>&quot;Arden Heights&quot;</td><td>&quot;Boro Zone&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 4)\n",
       "┌────────────┬───────────────┬─────────────────────────┬──────────────┐\n",
       "│ LocationID ┆ Borough       ┆ Zone                    ┆ service_zone │\n",
       "│ ---        ┆ ---           ┆ ---                     ┆ ---          │\n",
       "│ i32        ┆ str           ┆ str                     ┆ str          │\n",
       "╞════════════╪═══════════════╪═════════════════════════╪══════════════╡\n",
       "│ 1          ┆ EWR           ┆ Newark Airport          ┆ EWR          │\n",
       "│ 2          ┆ Queens        ┆ Jamaica Bay             ┆ Boro Zone    │\n",
       "│ 3          ┆ Bronx         ┆ Allerton/Pelham Gardens ┆ Boro Zone    │\n",
       "│ 4          ┆ Manhattan     ┆ Alphabet City           ┆ Yellow Zone  │\n",
       "│ 5          ┆ Staten Island ┆ Arden Heights           ┆ Boro Zone    │\n",
       "└────────────┴───────────────┴─────────────────────────┴──────────────┘"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zones_df = pl.read_parquet(\"../data/taxi_zone_lookup.parquet\")\n",
    "zones_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2b8b6e",
   "metadata": {},
   "source": [
    "This dataframe contains information about each location ID, and the `str` names for the Borough, Zone, and service zone that each location ID represents. Let's quickly rename the columns to make them snake_case like those of `march_yellow_rides_df`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7aac4ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>location_id</th><th>borough</th><th>zone</th><th>service_zone</th></tr><tr><td>i32</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>1</td><td>&quot;EWR&quot;</td><td>&quot;Newark Airport&quot;</td><td>&quot;EWR&quot;</td></tr><tr><td>2</td><td>&quot;Queens&quot;</td><td>&quot;Jamaica Bay&quot;</td><td>&quot;Boro Zone&quot;</td></tr><tr><td>3</td><td>&quot;Bronx&quot;</td><td>&quot;Allerton/Pelham Gardens&quot;</td><td>&quot;Boro Zone&quot;</td></tr><tr><td>4</td><td>&quot;Manhattan&quot;</td><td>&quot;Alphabet City&quot;</td><td>&quot;Yellow Zone&quot;</td></tr><tr><td>5</td><td>&quot;Staten Island&quot;</td><td>&quot;Arden Heights&quot;</td><td>&quot;Boro Zone&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 4)\n",
       "┌─────────────┬───────────────┬─────────────────────────┬──────────────┐\n",
       "│ location_id ┆ borough       ┆ zone                    ┆ service_zone │\n",
       "│ ---         ┆ ---           ┆ ---                     ┆ ---          │\n",
       "│ i32         ┆ str           ┆ str                     ┆ str          │\n",
       "╞═════════════╪═══════════════╪═════════════════════════╪══════════════╡\n",
       "│ 1           ┆ EWR           ┆ Newark Airport          ┆ EWR          │\n",
       "│ 2           ┆ Queens        ┆ Jamaica Bay             ┆ Boro Zone    │\n",
       "│ 3           ┆ Bronx         ┆ Allerton/Pelham Gardens ┆ Boro Zone    │\n",
       "│ 4           ┆ Manhattan     ┆ Alphabet City           ┆ Yellow Zone  │\n",
       "│ 5           ┆ Staten Island ┆ Arden Heights           ┆ Boro Zone    │\n",
       "└─────────────┴───────────────┴─────────────────────────┴──────────────┘"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zones_df = (\n",
    "    zones_df\n",
    "    .rename({\n",
    "        \"LocationID\": \"location_id\",\n",
    "        \"Borough\": \"borough\",\n",
    "        \"Zone\": \"zone\",\n",
    "    })\n",
    ")\n",
    "zones_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea448c9b",
   "metadata": {},
   "source": [
    "Perfect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af435413",
   "metadata": {},
   "source": [
    "If we want to now combine it with the summary statistics we just computed, we can do so by **joining** it into the summary statistics table, using `pl.DataFrame.join()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76339add",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    location_id_summary_df\n",
    "    .join(\n",
    "        zones_df,\n",
    "        left_on=\"pu_location_id\",\n",
    "        right_on=\"location_id\"\n",
    "    )\n",
    "    .head()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c575a651",
   "metadata": {},
   "source": [
    "Nice! The data from `zones_df` has been joined into our zone-summary table, and we can see the `\"borough\"`, `\"zone\"`, and `\"service_zone\"` alongside the `\"pu_location_id\"`! `polars`'s `.join()` functionality has all the same functionality you may be familiar with from Pandas and/or SQL:\n",
    "1. `location_id_summary_df` specifies the **left** table for the join.\n",
    "2. `.join(...)` starts the join.\n",
    "3. `zones_df` specifies the **right** table for the join.\n",
    "4. `left_on` specifies the column to join on from `location_id_summary_df`.\n",
    "5. `right_on` specifies the column to join on from `zones_df`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cb3caa",
   "metadata": {},
   "source": [
    "Just to be clear, though, we didn't have to do the aggregation to `location_id_summary_df`, first; we can join directly into the original `march_yellow_rides_df`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6e5102",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    march_yellow_rides_df\n",
    "    .select([\"tpep_pickup_datetime\", \"pu_location_id\", \"do_location_id\", \"trip_distance\", \"total_amount\"])\n",
    "    .join(\n",
    "        zones_df,\n",
    "        left_on=\"pu_location_id\",\n",
    "        right_on=\"location_id\",    \n",
    "    )\n",
    "#     .select([\"tpep_pickup_datetime\", \"pu_location_id\", \"do_location_id\", \"trip_distance\", \"total_amount\", \"borough\", \"zone\"])\n",
    "    .head()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86be53e4",
   "metadata": {},
   "source": [
    "But now, it's a bit strange that we have the `\"pu_location_id\"` and `\"do_location_id\"` in the same dataframe, because it's not clear whether the `\"borough\"` and `\"zone\"` refer to the `\"pu_location_id\"` or the `\"do_location_id\"`. We can quickly fix that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea321b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "zone_columns = [\"borough\", \"zone\", \"service_zone\"]\n",
    "(\n",
    "    march_yellow_rides_df\n",
    "    .select([\"tpep_pickup_datetime\", \"pu_location_id\", \"do_location_id\", \"trip_distance\", \"total_amount\"])\n",
    "    .join(\n",
    "        zones_df,\n",
    "        left_on=\"pu_location_id\",\n",
    "        right_on=\"location_id\",\n",
    "    )\n",
    "    .with_columns([pl.col(zone_columns).name.prefix(\"pu_\")])\n",
    "    .drop(zone_columns)\n",
    "    .head()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed42a13b",
   "metadata": {},
   "source": [
    "We can also use the `on` argument in case the name of the two columns is the same in both tables. For example, imagine we wanted to join `zones_df` with the table we created in the prior module, of rides that both started in the same location ID; then we don't need to specify pickup vs dropoff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a10288",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    march_yellow_rides_df\n",
    "    .filter(pl.col(\"do_location_id\").eq(pl.col(\"pu_location_id\")))\n",
    "    .select([\n",
    "        \"tpep_pickup_datetime\",\n",
    "        pl.col(\"pu_location_id\").alias(\"location_id\"),\n",
    "        \"trip_distance\",\n",
    "        \"total_amount\",\n",
    "    ])\n",
    "    .join(\n",
    "        zones_df,\n",
    "        on=\"location_id\",\n",
    "    )\n",
    "    .head()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5ba807",
   "metadata": {},
   "source": [
    "`polars` also supports all possible joins: anti joins, left joins, right joins, inner joins, outer joins... you've got it all, all you have to use is the `how` argument. To this end, there's a curious difference between `zones_df` and `march_yellow_rides_df`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a35c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of distinct pu_location_id in `march_yellow_rides_df`: {march_yellow_rides_df.select(pl.col('pu_location_id').n_unique())}\")\n",
    "print(f\"Number of distinct pu_location_id in `zones_df`: {zones_df.select(pl.col('location_id').n_unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6174d3",
   "metadata": {},
   "source": [
    "There are exactly **6** more `location_id`s in `zones_df` than there are `pu_location_id`s in `march_yellow_rides_df`! This makes sense--`zones_df` serves as an index of **all** zones, and there's no guarantee that all zones had at least one ride. What if we wanted to see which zones didn't have any rides that started in them (i.e. `location_id`s with no matching `pu_location_id`)? For that, we can use an anti-join:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012758c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    zones_df\n",
    "    .join(march_yellow_rides_df, right_on=\"pu_location_id\", left_on=\"location_id\", how=\"anti\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27871831",
   "metadata": {},
   "source": [
    "We can confirm, the result has a length of **6** data points; exactly what we saw in our `n_unique()` check."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315ee9ae",
   "metadata": {},
   "source": [
    "And, of course, as with the other Query Statements, we can join not only on a column name `str`, but also on a `pl.Expr` object. Consider the following toy example, where we want to join two columns together based on whether or not they are even or odd:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1edc95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_df = pl.DataFrame({\"a\": [1, 2, 3, 4], \"b\": [\"Danny\", \"Donna\", \"Lana\", \"Sauna\"]})\n",
    "right_df = pl.DataFrame({\"c\": [1, 2], \"d\": [\"cool\", \"not cool\"]})\n",
    "display(left_df)\n",
    "display(right_df)\n",
    "\n",
    "joined_df = (\n",
    "    left_df\n",
    "    .join(\n",
    "        right_df,\n",
    "        left_on=pl.col(\"a\") % 2,\n",
    "        right_on=pl.col(\"c\") % 2\n",
    "    )\n",
    ")\n",
    "display(joined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c791f4",
   "metadata": {},
   "source": [
    "Note, `2024-05-30`: At the time of writing, `polars` does not yet support non-equi joins. In other words, the join condition has to be an equality condition, like \"dataframe 1 column equals dataframe 2 column\". This is less flexible than join options offered by e.g. typical SQL, where you can join on a non-equality condition like \"dataframe 1 column is less than dataframe 2 column\". But not to fret! `polars` contributors are actively working on this; and you can stay tuned to the progress here: https://github.com/pola-rs/polars/issues/10068."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b8cae4",
   "metadata": {},
   "source": [
    "## 5.2. Concatenating Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea24a23",
   "metadata": {},
   "source": [
    "The data we've been working with so far has been the file `\"../data/yellow_tripdata_2024-03.parquet\"`, which contains all rides given by yellow taxis in NYC in the month of March, 2024. Amazingly, New York City records and publishes this data every month, going all the way back to 2009! What if we wanted to analyze two months' datasets together, in the same dataframe? Well, for that, we have `pl.concat()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c679c9",
   "metadata": {},
   "source": [
    "Let's start by loading in February's yellow taxi trip data. It should have the same schema as the data from March, so we can use the same column name mapping dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d377311c",
   "metadata": {},
   "outputs": [],
   "source": [
    "february_yellow_rides_df = (\n",
    "    pl.read_parquet(\"../data/yellow_tripdata_2024-02.parquet\")\n",
    "    .rename(yellow_rides_column_rename_mapping)\n",
    ")\n",
    "february_yellow_rides_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c31054e",
   "metadata": {},
   "source": [
    "To combine these two dataframes, we just have to pass them in as a list to `pl.concat()`, and we can see the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697e0907",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_yellow_rides_df = pl.concat([\n",
    "    february_yellow_rides_df,\n",
    "    march_yellow_rides_df,\n",
    "])\n",
    "display(yellow_rides_df.head())\n",
    "print(f\"{february_yellow_rides_df.shape[0]} rides recorded in `february_yellow_rides_df`.\")\n",
    "print(f\"{march_yellow_rides_df.shape[0]} rides recorded in `march_yellow_rides_df`.\")\n",
    "print(f\"{all_yellow_rides_df.shape[0]} rides recorded in `all_yellow_rides_df`.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d827fd08",
   "metadata": {},
   "source": [
    "And just like that, we have a combined dataframe of all the yellow taxi rides from February and March! Yellow taxis aren't the only type of taxis in New York City, though; for example, there are also green taxis, which the city of NYC also records. However, a slightly different taxi type means slightly different data... how will it be to combine such data together? Let's see..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6a03ec",
   "metadata": {},
   "source": [
    "Let's start by loading in `march_green_rides_df` from file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bfb98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "march_green_rides_df = pl.read_parquet(\"../data/green_tripdata_2024-03.parquet\")\n",
    "march_green_rides_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6a440e",
   "metadata": {},
   "source": [
    "Already, we can see some slight differences between the `all_yellow_rides_df` and `march_green_rides_df`--for example, `march_green_rides_df` has 20 columns, while we're accustomed to the 19 columns in `march_yellow_rides_df`--but there are also a lot of similarities. For one, `march_green_rides_df` has the same `datetime` columns (though with slighly different names), and the same TitleCase ID columns. We'll need to conform the column names so that the correct columns match up with each other upon concatenation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c754047",
   "metadata": {},
   "outputs": [],
   "source": [
    "green_rides_df_column_mapping = {\n",
    "    \"VendorID\": \"vendor_id\",\n",
    "    \"RatecodeID\": \"ratecode_id\",\n",
    "    \"PULocationID\": \"pu_location_id\",\n",
    "    \"DOLocationID\": \"do_location_id\",\n",
    "    \"lpep_pickup_datetime\": \"tpep_pickup_datetime\",\n",
    "    \"lpep_dropoff_datetime\": \"tpep_dropoff_datetime\",\n",
    "    # \"Airport_fee\": \"airport_fee\",  # Doesn't exist in `green_rides_df`\n",
    "}\n",
    "march_green_rides_df = march_green_rides_df.rename(green_rides_df_column_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e72b52",
   "metadata": {},
   "source": [
    "Great! Now, if we want to finally combine the yellow taxi dataframes and green taxi dataframe with `pl.concat()`, we just pass them in as a list again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196c4b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rides_df = (\n",
    "    pl.concat([\n",
    "        february_yellow_rides_df,\n",
    "        march_yellow_rides_df,\n",
    "        march_green_rides_df,\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22aaf010",
   "metadata": {},
   "source": [
    "Even with the renaming, we got an error! That's because, by default, the two dataframes have to have the same schema. This isn't the case in our dataframes, though; however we'd still like to concatenate them! Thankfully, `polars` gives us flexible control over this, with the `how` argument. By default, `how` is set to `how=\"vertical\"`, restricting the concatenation to a strict vertical stacking of the dataframes. However, we can set `how=\"diagonal\"`, and the two dataframes will be concatenated together, with nulls used to fill in the spaces of unshared columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a0a834",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rides_df = (\n",
    "    pl.concat(\n",
    "        [\n",
    "            february_yellow_rides_df,\n",
    "            march_yellow_rides_df,\n",
    "            green_rides_df,\n",
    "        ],\n",
    "        how=\"diagonal\"\n",
    "    )\n",
    ")\n",
    "all_rides_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec3a3d3",
   "metadata": {},
   "source": [
    "Scrolling to the right, we can see that `ehail_fee` and `trip_type` are null, since the yellow trips data doesn't have these two columns--they are brought over from `green_rides_df`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce2fa95",
   "metadata": {},
   "source": [
    "`pl.concat()` does offer a few more options, such as horizontal concatenation instead of vertical concatenation, but it's not so useful for us here; instead, I leave it to your reading!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbed8b23",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498d520e",
   "metadata": {},
   "source": [
    "In this module, we learned how to combine multiple dataframes together, using two new Query Statements, `.join()` and `.concat()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b7dd08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
