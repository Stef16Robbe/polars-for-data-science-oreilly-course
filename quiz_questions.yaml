- module_name: 00. Course Overview
  quiz_questions: []
- module_name: 01. Introduction to Polars
  quiz_questions:
  - answers:
    - is_correct: true
      rationale: Polars is implemented in Rust, and has bindings in other languages, all of which use the underlying Rust implementation.
      text: Rust
    - is_correct: false
      rationale: C is indeed a fast low-level language, but it's not the one that Polars is implemented in.
      text: C
    - is_correct: false
      rationale: Though Polars has Python bindings, it is implemented in a different language!
      text: Python
    - is_correct: false
      rationale: R is a great language for data science, but Polars is not implemented in R.
      text: R
    question: What language is Polars implemented in?
    question_name: Polars Implementation Language
    question_reference_note: To answer this question, see the question "Polars Implementation
      Language" in the notebook "01. Introduction to Polars - Quiz.ipynb".
    reference_video: 1.1 Introduction to Polars
  - answers:
    - is_correct: true
      rationale: Polars is best for single node computing, while tools like Spark or Modin are best for distributed computing.
      text: Single node computing.
    - is_correct: false
      rationale: Not really; tools like Spark or Modin are better suited to this use-case.
      text: Distributed computing.
    question: Which is the most common use-case for Polars?
    question_name: Most Common Polars Use-case
    question_reference_note: To answer this question, see the question "Common Polars Use-case"
      in the notebook "01. Introduction to Polars - Quiz.ipynb".
    reference_video: 1.1 Introduction to Polars
  - answers:
    - is_correct: false
      rationale: This is what you have to do with Modin and Ray for Pandas, not Polars.
      text: Specify a backend engine for distributing queries.
    - is_correct: false
      rationale: You don’t need to enter information regarding your OS's specifics into a configuration file for Polars to distribute resources.
      text: Enter some information regarding your OS's specifics into a configuration
        file which Polars will then use to distribute resources.
    - is_correct: true
      rationale: Polars is automatically configured such that its query optimization engine uses all cores available on the host machine.
      text: Nothing, it works like that right out of the box.
    - is_correct: false
      rationale: There is no need to specify the number of cores on the local machine when importing Polars.
      text: Specify the number of cores on the local machine when importing Polars.
    question: What do you have to do to configure Polars such that its query optimization
      engine uses all cores available on the host machine?
    question_name: Polars Multi-core Configuration
    question_reference_note: To answer this question, see the question "Polars Multi-core
      Configuration" in the notebook "01. Introduction to Polars - Quiz.ipynb".
    reference_video: 1.1 Introduction to Polars
  - answers:
    - is_correct: false
      rationale: Polars is implemented in Rust, but that is not the underlying memory
        model.
      text: Rust
    - is_correct: false
      rationale: Polars has bindings in Python, but that is not its underlying memory
        model.
      text: Python
    - is_correct: true
      rationale: Polars uses Apache Arrow for its underlying memory model
      text: Apache Arrow
    - is_correct: true
      rationale: Polars uses a memory model which optimized for Online Analytical
        Processing use-cases, but that is not the memory model itself.
      text: Online Analytical Processing
    question: What does Polars use as its underlying memory model?
    question_name: Polars Underlying Memory Model
    question_reference_note: To answer this question, see the question "Polars Underlying
      Memory Model" in the notebook "01. Introduction to Polars - Quiz.ipynb".
    reference_video: 1.1 Introduction to Polars
  - answers:
    - is_correct: true
      rationale: Exactly! Modern analytics use-cases usually involve column-oriented
        actions, so having data from the same column close together helps Apache Arrow
        to optimize for those use-cases.
      text: Data from the same column is placed close together in memory.
    - is_correct: false
      rationale: You might be thinking about OLTP, but Apache Arrow is
        optimized for OLAP.
      text: Data from the same row is placed close together in memory.
    - is_correct: false
      rationale: This is an interesting design consideration that differs across distributed
        computing tools like Hadoop and Spark, but it's not relevant here.
      text: Data's storage location is close to the data's processing location.
    - is_correct: false
      rationale: This isn’t relevant to how the Apache Arrow memory model is optimized for column-oriented OLAP use-cases.
      text: Data gets sorted before processing.
    question: How is the Apache Arrow memory model optimized for column-oriented OLAP
      use-cases?
    question_name: Apache Arrow Memory Model Optimization
    question_reference_note: To answer this question, see the question "Apache Arrow Memory
      Model Optimization" in the notebook "01. Introduction to Polars - Quiz.ipynb".
    reference_video: 1.2 Apache Arrow - A Brief Intro
- module_name: 02. Getting Started
  quiz_questions:
  - answers:
    - is_correct: false
      rationale: If you want the last column to be a categorical variable,
        then you'd need to perform some additional type-casting on the column.
      text: (`str`, `str`, `f64`, `cat`)
    - is_correct: true
      rationale: Columns 1, 2, and 4 are strings, but column 3, having at
        least one value with a decimal point, gets cast as a float.
      text: (`str`, `str`, `f64`, `str`)
    - is_correct: false
      rationale: If you want the last column to be a categorical variable, then you'd need to perform some additional type-casting on the column; also, are you sure there’s i64 data?
      text: (`str`, `str`, `i64`, `cat`)
    - is_correct: false
      rationale: This option is missing a column.
      text: (`str`, `i64`, `cat`)
    question: Given the following data dictionary about school children, create a
      `pl.DataFrame` and display it. What are the datatypes of each column?
    reference_video: 2.1 Creating a Polars DataFrame
  - answers:
    - is_correct: false
      rationale: This would only happen if you don't pass a `schema_overrides` argument
        to the function call. Try again, perhaps you didn't enter the code correctly.
      text: All the data is loaded, and the columns `tpep_pickup_datetime` and `tpep_dropoff_datetime`
        are loaded as `str` datatype.
    - is_correct: false
      rationale: This would only happen if you passed `pl.Datetime` as the schema
        override; we're trying to override with `pl.Date`!.
      text: All the data is loaded, and the columns `tpep_pickup_datetime` and `tpep_dropoff_datetime`
        are loaded as `datetime` datatype.
    - is_correct: true
      rationale: Polars crashes if you try to force a schema that it cannot conform the data to!
      text: The data doesn't load.
    - is_correct: false
      rationale: It would be nice if this happened, but unfortunately there are some
        complications... try to run the code again!
      text: All the data is loaded, and the columns `tpep_pickup_datetime` and `tpep_dropoff_datetime`
        are loaded as `date` datatype.
    question: In the module, we loaded data from the csv file, overriding the schema
      of the columns `tpep_pickup_datetime` and `tpep_dropoff_datetime`, loading them
      as a `pl.Datetime` data type. Now, override the schema to load them as a `pl.Date`
      data type. What happens?
    reference_video: 2.2 Reading Data From CSV with In-Memory Mode
  - answers:
    - is_correct: false
      rationale: Though csv is a simpler file type, it's not optimized for column
        operations!
      text: '`csv`, because it''s a simpler file type.'
    - is_correct: false
      rationale: Though csv is indeed an older file format, parquet still offers a
        greater speedup here!
      text: '`csv`, because it''s an older file format, so the Polars code for interacting
        with it is better developed.'
    - is_correct: false
      rationale: This may or may not be true, but either way it's not the real reason
        why parquet offers a greater speedup on in-memory vs lazy selection.
      text: '`parquet`, because, since Polars is built on the Apache Arrow memory
        model, the development team has spent more time developing the functionality
        associated with parquet, which is also built on the Apache Arrow memory model,
        thus making its IO operations faster.'
    - is_correct: true
      rationale: Exactly! This is what Apache Arrow is all about!
      text: '`parquet`, because `parquet` files keep data from the same column in
        the same location in memory, so when the `select` gets pushed down to the
        read operation of `LazyFrame`, the input engine can skip the unnecessary columns''
        data faster than it can for `csv`.'
    question: 'During the module, we saw that selecting columns from a `LazyFrame`
      was ~2-3x faster than selecting columns from a `DataFrame`, when data is loaded
      from a `csv`. However, we only did this for csv, not for parquet. Which file
      type do you think would have a greater speedup from selecting on a `DataFrame`
      to selecting on a `LazyFrame`: `csv` or `parquet`? Why?'
    reference_video: 2.4 Selecting Data - In-Memory vs Lazy Mode Comparison
  - answers:
    - is_correct: false
      rationale: That is the number of rows in the dataset, are you sure you're reading
        the correct row in the `describe` table?
      text: '3582628'
    - is_correct: true
      rationale: Exactly! There are a few columns with exactly this amount of nulls.
        We'll get more into this later...
      text: '426190'
    - is_correct: false
      rationale: We're looking for the highest `null_count` that any column has, not
        the lowest!
      text: '0'
    - is_correct: false
      rationale: Are you sure you're looking at the right place in the table?
      text: '176836'
    question: Inspect the dataset with `df.describe()`. What is the highest `null_count`
      that any column has?
    reference_video: 2.2 Reading Data From CSV with In-Memory Mode
- module_name: 03. Data Manipulation I - Basics
  quiz_questions:
  - answers:
    - is_correct: false
      rationale: Are you sure you're checking for "greater than 0" with the right
        function? Try using `.gt()`!
      text: '`int64`'
    - is_correct: false
      rationale: This is the original datatype of the `tolls_amount` column... are
        you sure your function worked correctly?
      text: '`float64`'
    - is_correct: false
      rationale: Are you sure you are performing the operation on the correct column?
      text: '`str`'
    - is_correct: true
      rationale: The function `.gt()` will return a boolean--true if greater than, false if not.
      text: '`bool`'
    question: 'Using `.select()`, fetch a column from `df` which represents whether
      or not a toll was paid as part of the trip. What is the datatype of that new
      column? (Hint: you can check if a toll was paid by seeing if `tolls_amount`
      is greater than 0.)'
    question_name: Toll Payment Column Datatype
    question_reference_note: To answer this question, see the question "Toll Payment Column
      Datatype" in the notebook "03. Data Manipulation I - Basics - Quiz.ipynb".
    reference_video: 3.2 Getting Started with Column Expressions
  - answers:
    - is_correct: true
      rationale: This is precisely the result you get when you filter then take the
        max. And it's quite a long trip indeed!
      text: '176836.3'
    - is_correct: false
      rationale: You might be using `.min()` instead of `.max()` by accident...
      text: '0'
    - is_correct: false
      rationale: Make sure to check for rides for which `tolls_amount` was equal to
        0, not rides where `tolls_amount` is greater than 0!
      text: '176744.79'
    - is_correct: false
      rationale: Are you sure you're using the `tolls_amount` column and not accidentally
        the `tip_amount` column?
      text: '176329.23'
    question: 'What was the longest trip that had `0` tolls paid (hint: use `.filter()`
      to get only the trips with `tolls_amount` equal to `0`, and `.select()` with
      `.max()` to find the longest trip)?'
    question_name: Longest Trip with Zero Tolls
    question_reference_note: To answer this question, see the question "Longest Trip with
      Zero Tolls" in the notebook "03. Data Manipulation I - Basics - Quiz.ipynb".
    reference_video: 3.2 Getting Started with Column Expressions
  - answers:
    - is_correct: false
      rationale: We're looking for the mean `tip_amount` for trips where the `fare_amount`
        was greater than `$20`, not the mean `fare_amount` for trips where the `fare_amount`
        was greater than `$20`!
      text: '39.32003'
    - is_correct: false
      rationale: That's a big tip amount! You might accidentally be taking the maximum
        tip amount rather than the mean tip amount...
      text: '598.58'
    - is_correct: false
      rationale: Make sure you're using the `.gt` function ("greater than") and not
        the `.ge` function ("greater than or equal to")!
      text: '5.870243'
    - is_correct: true
      rationale: This is indeed the number of trips with a fare amount greater than
        $20--you correctly used `gt` and not `ge`.
      text: '5.872741'
    question: What is the mean tip amount for trips where the fare amount was greater
      than $20?
    question_name: Mean Tip for High Fare Trips
    question_reference_note: To answer this question, see the question "Mean Tip for High
      Fare Trips" in the notebook "03. Data Manipulation I - Basics - Quiz.ipynb".
    reference_video: 3.2 Getting Started with Column Expressions
  - answers:
    - is_correct: false
      rationale: That's the maximum `trip_distance` for trips that had a `passenger_count`
        of exactly 2... Check your code again!
      text: '159.74'
    - is_correct: true
      rationale: Precisely, and it's a long trip indeed! There are certainly some
        outliers in this dataset.
      text: '66907.9'
    - is_correct: false
      rationale: That's the mean `trip_distance` for trips with a passenger count
        of 1 or 2, but we're looking for the maximum!
      text: '3.530788'
    - is_correct: false
      rationale: That's the maximum `total_amount` for trips with a passenger count
        of 1 or 2, but we're looking for the maximum `trip_distance`!
      text: '1021.99'
    question: Find the maximum trip distance for trips with a passenger count of 1
      or 2.
    question_name: Max Trip Distance for 1-2 Passengers
    question_reference_note: To answer this question, see the question "Max Trip Distance
      for 1-2 Passengers" in the notebook "03. Data Manipulation I - Basics - Quiz.ipynb".
    reference_video: 3.3 The .filter() Query Statement
  - answers:
    - is_correct: false
      rationale: You might be sorting in ascending order rather than descending order...
      text: 0.0, 159.74
    - is_correct: true
      rationale: Exactly! This is the result when you sort by `total_amount` descending.
      text: 3.8, 181.5
    - is_correct: false
      rationale: These are the two highest values for `total_amount`, but we are looking
        for the values of `trip_distance` associated with two highest values of `total_amount`!
      text: 1021.99, 951.26
    - is_correct: false
      rationale: Are you sure you're looking at the right columns?
      text: 5.1, 8.3
    question: Sort the dataframe by `total_amount` in descending order; then, select
      and display only the top 5 rows and the columns `trip_distance` and `total_amount`.
      What are the two values of `trip_distance` associated with the two trips with
      the highest `total_amount`?
    question_name: Top 5 Trips by Total Amount
    question_reference_note: To answer this question, see the question "Top 5 Trips by Total
      Amount" in the notebook "03. Data Manipulation I - Basics - Quiz.ipynb".
    reference_video: 3.4 The .sort() Query Statement
  - answers:
    - is_correct: true
      rationale: This problem is solved with `.filter()` then `.select(...max())`,
        and it is a high fare amount indeed!
      text: '472.0'
    - is_correct: false
      rationale: We're looking for the maximum fare amount for trips that had a `tip_amount`
        greater than `$10` and a `trip_distance` greater than `10` miles, not either
        or!
      text: '900.0'
    - is_correct: false
      rationale: We're looking for the maximum fare amount, not the average!
      text: '66.124137'
    - is_correct: false
      rationale: Are you sure you're looking at the right column?
      text: '633.3'
    question: Calculate the maximum fare amount for trips that had a `tip_amount`
      greater than `$10` and a `trip_distance` greater than `10` miles.
    question_name: Maximum Fare for Long Trips with High Tips
    question_reference_note: To answer this question, see the question "Maximum Fare for
      Long Trips with High Tips" in the notebook "03. Data Manipulation I - Basics
      - Quiz.ipynb".
    reference_video: 3.3 The .filter() Query Statement
  - answers:
    - is_correct: false
      rationale: Are you sure you're filtering for just trips with a distance greater
        than 30, and not filtering for trips with a distance greater than 300?
      text: '0.034265'
    - is_correct: false
      rationale: This is the average `price_per_mile`, not the maximum!
      text: '4.868929'
    - is_correct: true
      rationale: You correctly used `.filter()`, column arithmetic to compute `price_per_mile`,
        and finally `.max` to get the answer!
      text: '14.067142'
    - is_correct: false
      rationale: Are you sure you're looking at the right columns?
      text: '9.384736'
    question: Find the maximum `price_per_mile` (by dividing the `total_amount` by
      the `trip_distance`) for trips with a distance greater than `30`.
    question_name: Maximum Price per Mile for Long Trips
    question_reference_note: To answer this question, see the question "Maximum Price per
      Mile for Long Trips" in the notebook "03. Data Manipulation I - Basics - Quiz.ipynb".
    reference_video: 3.3 The .filter() Query Statement
  - answers:
    - is_correct: true
      rationale: Exactly! Only a few trips had a `trip_distance` of exactly 5, and
        this is the latest one.
      text: '2024-03-31 23:46:21'
    - is_correct: false
      rationale: Remember--we're looking for maximum pickup datetime, not dropoff
        datetime!
      text: '2024-03-31 23:57:07'
    - is_correct: false
      rationale: Remember--we're looking for maximum pickup datetime, not minimum!
      text: '2024-03-01 00:05:00'
    - is_correct: false
      rationale: Are you sure your filtering for only trips that had a `trip_distance`
        of exactly 5?
      text: '2024-04-01 00:34:55'
    question: Of all the trips which had a `trip_distance` of exactly `5`, what was
      the latest `tpep_pickup_datetime`?
    question_name: Latest Pickup for 5-Mile Trips
    question_reference_note: To answer this question, see the question "Latest Pickup for
      5-Mile Trips" in the notebook "03. Data Manipulation I - Basics - Quiz.ipynb".
    reference_video: 3.3 The .filter() Query Statement
  - answers:
    - is_correct: false
      rationale: Are you sure you're using the right columns for calculating the tip
        percentage of each ride?
      text: '0.01'
    - is_correct: false
      rationale: We're looking for the minimum tip, not the average tip!
      text: '0.276066'
    - is_correct: false
      rationale: Don't forget to filter the data!
      text: '-40.0'
    - is_correct: true
      rationale: You correctly used `.filter()`, column arithmetic to compute tip
        percentage, and finally `.min` to get the answer!
      text: '0.00003'
    question: 'Filtering only for trips that had a `fare_amount` and a `tip_amount`
      greater than 0, what was the lowest tip percentage (expressed as a fraction)
      that somebody paid? (Hint: divide `tip_amount` by `fare_amount`).'
    question_name: Lowest Tip Percentage
    question_reference_note: To answer this question, see the question "Lowest Tip Percentage"
      in the notebook "03. Data Manipulation I - Basics - Quiz.ipynb".
    reference_video: 3.3 The .filter() Query Statement
  - answers:
    - is_correct: false
      rationale: Make sure to check for rides where the `tip_amount` is greater than
        `Airport_fee`, not rides where `tip_amount` is greater than or equal to `Airport_fee`!
      text: '3089862'
    - is_correct: false
      rationale: That's the total amount of rides in the dataset! Don't forget to
        filter.
      text: '3582628'
    - is_correct: true
      rationale: You correctly used `.filter()`, and then any number of ways to check
        the dataframe shape--`.shape`, `display()`, `print()`...
      text: '2461463'
    - is_correct: false
      rationale: Are you sure you're using the correct columns?
      text: '982746'
    question: How many trips had a `tip_amount` greater than the `Airport_fee`?
    question_name: Trips with Tips Exceeding Airport Fee
    question_reference_note: To answer this question, see the question "Trips with Tips
      Exceeding Airport Fee" in the notebook "03. Data Manipulation I - Basics - Quiz.ipynb".
    reference_video: 3.2 Getting Started with Column Expressions
- module_name: 04. Data Manipulation II - Advanced Selecting
  quiz_questions:
  - answers:
    - is_correct: true
      rationale: This is like what we saw in the module! Only one column in our dataframe
        is a string data type.
      text: '1'
    - is_correct: false
      rationale: These are all the columns in the original dataframe! Are you sure
        you have your `.select()` statement correct?
      text: '19'
    - is_correct: false
      rationale: Are you sure you're not checking for a datatype other than `pl.String`?
      text: '3'
    - is_correct: false
      rationale: There is at least one column with the `pl.String` data type... check
        again!
      text: '0'
    question: Select all columns from the dataframe that have the datatype `pl.String`.
      How many columns are there in the result?
    question_name: String Columns Count
    question_reference_note: To answer this question, see the question "String Columns Count"
      in the notebook "04. Data Manipulation II - Advanced Selecting - Quiz.ipynb".
    reference_video: 4.1 Operating on Multiple Columns at the Same Time
  - answers:
    - is_correct: false
      rationale: These are all the columns in the original dataframe! Are you sure
        you have your `.select()` statement correct?
      text: '19'
    - is_correct: false
      rationale: Are you sure you're not checking for a datatype other than `pl.Int64`?
      text: '1'
    - is_correct: true
      rationale: This is like what we saw in the module! Three columns in our dataframe
        have a pl.Int64 data type.
      text: '3'
    - is_correct: false
      rationale: This is the number of rows, not the number of columns!
      text: '3582628'
    question: Select all columns from the dataframe that have the datatype `pl.Int64`.
      How many columns are there in the result?
    question_name: Int64 Columns Count
    question_reference_note: To answer this question, see the question "Int64 Columns Count"
      in the notebook "04. Data Manipulation II - Advanced Selecting - Quiz.ipynb".
    reference_video: 4.1 Operating on Multiple Columns at the Same Time
  - answers:
    - is_correct: false
      rationale: Make sure you're using `pl.Float64` and not another datatype.
      text: '0'
    - is_correct: true
      rationale: There are a few ways you could have gotten to this answer; most straightforward
        was to construct a column expression like 'has at least one null float64 value'
        and take the `.mean()`.
      text: '0.000076'
    - is_correct: false
      rationale: Are you sure you're using the correct datatype?
      text: '.50'
    - is_correct: false
      rationale: This is all the rows! Check your code again, maybe you have the aggregation
        incorrect...
      text: '1.0'
    question: 'What fraction of rows have at least one of their `pl.Float64` columns
      equal to exactly `0`? (Hint: use `pl.any_horizontal()`.)'
    question_name: Rows with Zero Float64 Values
    question_reference_note: To answer this question, see the question "Rows with Zero Float64
      Values" in the notebook "04. Data Manipulation II - Advanced Selecting - Quiz.ipynb".
    reference_video: 4.1 Operating on Multiple Columns at the Same Time
  - answers:
    - is_correct: false
      rationale: You might have the two columns mixed up!
      text: '`tip_amount = 17.0`, `fare_amount = 999.99`'
    - is_correct: false
      rationale: Make sure to take sort in descending order, not ascending!
      text: '`tip_amount = 0.01`, `fare_amount = -800.0`'
    - is_correct: true
      rationale: This is quite a high tip amount! It's likely some noise, there is
        a lot of that in the dataset.
      text: '`tip_amount = 999.99`, `fare_amount = 17.0`'
    - is_correct: false
      rationale: Are you sure you're computing the column `tip_amount_plus_fare_amount`
        correctly?
      text: '`tip_amount = 999.99`, `fare_amount = 999.99`'
    question: Create a new column called `tip_amount_plus_fare_amount`; sort the dataframe
      by this new column descending order. What is the `tip_amount` and `fare_amount`
      for the highest `tip_amount_plus_fare_amount`?
    question_name: Highest Tip and Fare for New Column
    question_reference_note: To answer this question, see the question "Highest Tip and
      Fare for New Column" in the notebook "04. Data Manipulation II - Advanced Selecting
      - Quiz.ipynb".
    reference_video: 4.1 Adding New Columns with .with_columns()
  - answers:
    - is_correct: false
      rationale: Close... the answer is actually 1 minus this number! Check your code
        again.
      text: '0.949397'
    - is_correct: true
      rationale: There are a few ways you could answer this; most straightforward
        was to construct a column expression of 'same-pickup-dropoff' and take the
        `.mean()`.
      text: '0.050603'
    - is_correct: false
      rationale: This would mean that all the rides had the same pickup and dropoff
        location; try again!
      text: '1.0'
    - is_correct: false
      rationale: This would mean that none of the rides had the same pickup and dropoff
        location; try again!
      text: '0.0'
    question: What fraction of rides had the same pickup and dropoff location?
    question_name: Same Pickup-Dropoff Fraction
    question_reference_note: To answer this question, see the question "Same Pickup-Dropoff
      Fraction" in the notebook "04. Data Manipulation II - Advanced Selecting - Quiz.ipynb".
    reference_video: 4.1 Adding New Columns with .with_columns()
  - answers:
    - is_correct: false
      rationale: This is the number of rows, not columns; try again!
      text: '3582628'
    - is_correct: true
      rationale: There are 19 columns in the dataframe originally, so by adding again
        each column with just a new name, it doubles to 38!
      text: '38'
    - is_correct: false
      rationale: This is the the number of columns in the original dataframe; the
        answer should actually be two times this number!
      text: '19'
    - is_correct: false
      rationale: Are you sure you selected all the columns in your `.with_columns()`
        call?
      text: '22'
    question: Using `.with_columns()`, `pl.all()`, and `.name.suffix()`, add to the
      dataframe a copy of all the columns, just with the name `_new` added on to the
      end of each column name. How many columns are there in the resultant dataframe?
    question_name: Duplicate Columns with Suffix
    question_reference_note: To answer this question, see the question "Duplicate Columns
      with Suffix" in the notebook "04. Data Manipulation II - Advanced Selecting
      - Quiz.ipynb".
    reference_video: 4.1 Adding New Columns with .with_columns()
  - answers:
    - is_correct: false
      rationale: Are you using `.select()`? Be sure to use `.with_columns()` since
        we are adding columns.
      text: '1'
    - is_correct: true
      rationale: Exactly! There's only one column with the datatype `pl.String`.
      text: '20'
    - is_correct: false
      rationale: Are you sure you're performing the operations on `pl.String` columns?
      text: '22'
    - is_correct: false
      rationale: You may have tried something with `pl.all()`. Try again!
      text: '38'
    question: Add a new column to the dataframe for every `pl.String` column that
      checks if that column has an empty string (i.e. equal to `""`). How many columns
      are in the resultant dataframe?
    question_name: Empty String Check Columns
    question_reference_note: To answer this question, see the question "Empty String Check
      Columns" in the notebook "04. Data Manipulation II - Advanced Selecting - Quiz.ipynb".
    reference_video: 4.1 Adding New Columns with .with_columns()
  - answers:
    - is_correct: true
      rationale: Exactly! A small fraction of the rides had the same pickup and dropoff
        location.
      text: (181291, 18)
    - is_correct: false
      rationale: This is the number of rows in the original dataframe; are you making
        sure to filter appropriately?
      text: (3582628, 18)
    - is_correct: false
      rationale: Did you make sure to remove either the pickup or dropoff location
        column from the dataframe?
      text: (181291, 19)
    - is_correct: false
      rationale: Are you sure you didn't accidentally take only trips where the dropoff
        location and pickup location are not equal?
      text: (3401337, 18)
    question: We'd like a dataframe of only rides that took place in one location
      (i.e. where `DOLocationID` equals `PULocationID`). This also means that we no
      longer need both of the columns `DOLocationID` and `PULocationID` (since they
      are equal). So, filter for same-pickup-dropoff trips, and remove either one
      of the pickup/dropoff location columns, and rename the other one to just be
      `LocationID`. What is the shape of the resultant dataframe?
    question_name: Same Location Trips Dataframe
    question_reference_note: To answer this question, see the question "Same Location Trips
      Dataframe" in the notebook "04. Data Manipulation II - Advanced Selecting -
      Quiz.ipynb".
    reference_video: 4.4 Renaming Columns with .rename()
- module_name: 05. Data Manipulation III - Grouping and Aggregation
  quiz_questions:
  - answers:
    - is_correct: true
      rationale: Exactly! The maximum trip distance for trips with `pu_location_id
        = 1` is 35.75.
      text: '35.75'
    - is_correct: false
      rationale: Are you sure you chose the right `pu_location_id`?
      text: '29.7'
    - is_correct: false
      rationale: Make sure to use `pu_location_id`, not `do_location_id`!
      text: '176744.79'
    - is_correct: false
      rationale: Are you taking the minimum or the maximum?
      text: '0.0'
    question: What is the maximum trip distance for trips with `pu_location_id = 1`?
    question_name: Maximum Trip Distance for Specific Location
    question_reference_note: To answer this question, see the question "Maximum Trip Distance
      for Specific Location" in the notebook "05. Data Manipulation III - Grouping
      and Aggregation - Quiz.ipynb".
    reference_video: 4.4 Renaming Columns with .rename()
  - answers:
    - is_correct: false
      rationale: Are you sure you sorted in the right direction?
      text: '207'
    - is_correct: false
      rationale: Be sure to sort by mean_fare_amount then mean_trip_distance, and
        not the other way around!
      text: '205'
    - is_correct: false
      rationale: Are you sure you're using `mean` and not `max`?
      text: '265'
    - is_correct: true
      rationale: Exactly! `vendor_id = 6` has a much higher `mean_fare_amount` than
        the other vendors.
      text: '6'
    question: Group the data by `vendor_id` and calculate the average `fare_amount`
      and average `trip_distance` for each. Sort descending by both `mean_fare_amount`
      and `mean_trip_distance`. What is the top `vendor_id`?
    question_name: Top Vendor by Average Fare and Trip Distance
    question_reference_note: To answer this question, see the question "Top Vendor by Average
      Fare and Trip Distance" in the notebook "05. Data Manipulation III - Grouping
      and Aggregation - Quiz.ipynb".
    reference_video: 5.1 Grouping DataFrames with .group_by()
  - answers:
    - is_correct: false
      rationale: Make sure to get the date with the most rides, not the least.
      text: '2002-12-31'
    - is_correct: false
      rationale: We're looking for the date with the most rides, not the datetime
        with the most rides.
      text: '2024-03-14 22:04:00'
    - is_correct: true
      rationale: Exactly! There were 140383 rides on this day, more than on any other
        day.
      text: '2024-03-09'
    - is_correct: false
      rationale: Are you sure you're taking the maximum of the correct column?
      text: '2024-04-01'
    question: Which date for `tpep_pickup_datetime` had the most rides?
    question_name: Date with Most Rides
    question_reference_note: To answer this question, see the question "Date with Most Rides"
      in the notebook "05. Data Manipulation III - Grouping and Aggregation - Quiz.ipynb".
    reference_video: 5.1 Grouping DataFrames with .group_by()
  - answers:
    - is_correct: false
      rationale: Be sure to take the mean, not the max.
      text: '850.0'
    - is_correct: true
      rationale: Exactly! According to the pivot table, this answer is similar across
        most `payment_type`/`vendor_id` combinations.
      text: '18.390497'
    - is_correct: false
      rationale: Are you sure your choice of `values` is `fare_amount`?
      text: '3.411724'
    - is_correct: false
      rationale: Close! but the question is about precisely `vendor_id` 2 and `payment_type` 2.
      text: '18.582571'
    question: Create a pivot table that shows the average fare amount for each combination
      of `VendorID` and `payment_type`. What is the average fare amount associated
      with `vendor_id` 2 and `payment_type` 2?
    question_name: Average Fare by Vendor and Payment Type
    question_reference_note: To answer this question, see the question "Average Fare by
      Vendor and Payment Type" in the notebook "05. Data Manipulation III - Grouping
      and Aggregation - Quiz.ipynb".
    reference_video: 5.3 Pivot Tables with .pivot()
  - answers:
    - is_correct: false
      rationale: Are you sure you're taking the average trip_distance and not the
        maximum?
      text: '115.3'
    - is_correct: false
      rationale: Make sure to check for `vendor_id = 1`, not `2`.
      text: '13.309613'
    - is_correct: true
      rationale: 'Exactly right! Hint: you could have also solved his by using `.filter()`
        then `.mean()`.'
      text: '12.570663'
    - is_correct: false
      rationale: Make sure to check for rides that had an airport fee, not ones that
        didn't.
      text: '2.409085'
    question: Create a pivot table that shows the average trip distance for every
      combination of `vendor_id` and whether or not the ride has an airport fee. What
      is the average trip distance for rides with `vendor_id = 1` that have an airport
      fee?
    question_name: Average Trip Distance for Airport Fee Rides
    question_reference_note: To answer this question, see the question "Average Trip Distance
      for Airport Fee Rides" in the notebook "05. Data Manipulation III - Grouping
      and Aggregation - Quiz.ipynb".
    reference_video: 5.3 Pivot Tables with .pivot()
  - answers:
    - is_correct: false
      rationale: Are you sure you're taking the average trip_distance and not the
        maximum?
      text: '115.3'
    - is_correct: false
      rationale: Make sure to check for `vendor_id = 1`, not `2`.
      text: '13.309613'
    - is_correct: true
      rationale: 'Exactly right! Hint: you could have also used a pivot_table to solve
        this.'
      text: '12.570663'
    - is_correct: false
      rationale: Make sure to check for rides that had an airport fee, not ones that
        didn't.
      text: '2.409085'
    question: What is the average trip distance for rides with `vendor_id = 1` that
      have an airport fee? Use `filter` to include only data that's `vendor_id = 1`
      and `airport_fee > 0`, and then `select` to measure the average `trip_distance`.
    question_name: Average Trip Distance for Airport Fee Rides (Alternative Method)
    question_reference_note: To answer this question, see the question "Average Trip Distance
      for Airport Fee Rides (Alternative Method)" in the notebook "05. Data Manipulation
      III - Grouping and Aggregation - Quiz.ipynb".
    reference_video: 5.1 Grouping DataFrames with .group_by()
  - answers:
    - is_correct: true
      rationale: Exactly! Using `.rank().over()` we can create a column that represents
        the `trip_distance` rank within each `vendor_id`, filter for the 10th largest
        `trip_distance`s, and then sum.
      text: '113916.29'
    - is_correct: false
      rationale: Are you sure you're taking the 10th largest trip distance, and not
        the 9th?
      text: '115599.84'
    - is_correct: false
      rationale: Make sure to correctly set the `descending` argument in the `rank`
        function.
      text: '0.76'
    - is_correct: false
      rationale: Are you sure you're using `trip_distance` and not `total_amount`?
      text: '1178.31'
    question: Using `rank().over()`, what is the sum of each `vendor_id`s 10th largest
      `trip_distance`s, summed over all `vendor_id`s?
    question_name: Sum of 10th Largest Trip Distances by Vendor
    question_reference_note: To answer this question, see the question "Sum of 10th Largest
      Trip Distances by Vendor" in the notebook "05. Data Manipulation III - Grouping
      and Aggregation - Quiz.ipynb".
    reference_video: 5.2 Window Functions in Polars
  - answers:
    - is_correct: false
      rationale: Make sure to take only the top three `pu_location_id`s.
      text: 6.0735e6
    - is_correct: false
      rationale: You might be taking the three `pu_location_id`s with the lowest maximum
        `trip_distance`, not the highest maximum `trip_distance`.
      text: '891.48'
    - is_correct: true
      rationale: Nice! There are two aggregations involved here--the first is to group
        by `pu_location_id`, then take only the top three `pu_location_id`s by `trip_distance`,
        and then again add the `total_amount`s across those three `pu_location_id`s.
      text: 4.6460e6
    - is_correct: false
      rationale: Remember, you want the sum `total_amount` of the three `pu_location_id`s
        with the highest `max_trip_distance`, not the highest sum `total_amount`.
      text: 2.3340e7
    question: What is the sum of the `total_amount`s for all rides taken with one
      of the three `pu_location_id`s with the highest maximum `trip_distance`?
    question_name: Total Amount for Top 3 Pickup Locations
    question_reference_note: To answer this question, see the question "Total Amount for
      Top 3 Pickup Locations" in the notebook "05. Data Manipulation III - Grouping
      and Aggregation - Quiz.ipynb".
    reference_video: 5.1 Grouping DataFrames with .group_by()
  - answers:
    - is_correct: false
      rationale: There is no query optimization in in-memory mode.
      text: They are the same, because the query optimization engine doesn't care
        about the order of operations.
    - is_correct: true
      rationale: Since group-by is a relatively expensive operation, reducing
        the amount of data it needs to operate on speeds up performance!
      text: Filtering before the group-by is faster, because you reduce the amount
        of data handled by the group by operation.
    - is_correct: false
      rationale: Filtering is actually not an expensive operation; grouping is far
        more expensive.
      text: Filtering after the group-by is faster, because the computer doesn't have
        to worry about the expensive filter operation until the end.
    - is_correct: false
      rationale: Though it might be true that there are less rows to filter out, the
        true bottleneck of the operation is the grouping itself!
      text: Filtering after the group-by is faster, since the filter occurs on grouped
        data, thus it has less total rows to filter out.
    question: 'Sometimes we want to both filter and group data; for example, in this
      question we want to both group by `pu_location_id` and view the results for
      just one `pu_location_id`. In these cases, we can filter first or group by first
      and get the same result. So which is faster, and why--grouping then filtering,
      or filtering and grouping? Perform the following timing tests to get the answer,
      and choose the best explanation. (Note: we are in in-memory mode here.)'
    question_name: 'Performance Comparison: Filtering vs Grouping (In-Memory)'
    question_reference_note: 'To answer this question, see the question "Performance Comparison:
      Filtering vs Grouping (In-Memory)" in the notebook "05. Data Manipulation III
      - Grouping and Aggregation - Quiz.ipynb".'
    reference_video: 5.1 Grouping DataFrames with .group_by()
  - answers:
    - is_correct: true
      rationale: After the query hits the query optimization engine, they become the
        same query.
      text: They are the same, because the query optimization engine doesn't care
        about the order of operations.
    - is_correct: false
      rationale: This might be the case in in-memory mode, but not in lazy mode!
      text: Filtering before the group-by is faster, because you reduce the amount
        of data handled by the group by operation.
    - is_correct: false
      rationale: Filtering is actually not an expensive operation; grouping is far
        more expensive. Either way, it doesn't apply in lazy mode!
      text: Filtering after the group-by is faster, because the computer doesn't have
        to worry about the expensive filter operation until the end.
    - is_correct: false
      rationale: Though it might be true that there are less rows to filter out, the
        true bottleneck of the operation is the grouping itself. Regardless, the query
        optimization engine makes this irrelevant in lazy mode!
      text: Filtering after the group-by is faster, since the filter occurs on grouped
        data, thus it has less total rows to filter out.
    question: 'Sometimes we want to both filter and group data; for example, in this
      question we want to both group by `pu_location_id` and view the results for
      just one `pu_location_id`. In these cases, we can filter first or group by first
      and get the same result. So which is faster, and why--grouping then filtering,
      or filtering and grouping? Perform the following timing tests to get the answer,
      and choose the best explanation. (Note: we are in lazy mode here.)'
    question_name: 'Performance Comparison: Filtering vs Grouping (Lazy Mode)'
    question_reference_note: 'To answer this question, see the question "Performance Comparison:
      Filtering vs Grouping (Lazy Mode)" in the notebook "05. Data Manipulation III
      - Grouping and Aggregation - Quiz.ipynb".'
    reference_video: 5.1 Grouping DataFrames with .group_by()
- module_name: 06. Data Manipulation IV - Combining Data
  quiz_questions:
  - answers:
    - is_correct: true
      rationale: We get this by joining the `zones_df` into the `rides_df` as in the
        module, and grouping by the combination of `pu_zone` and `do_zone`.
      text: (Upper East Side South, Upper East Side North)
    - is_correct: false
      rationale: Make sure to get the `pu_zone` `do_zone` pair with the most rides,
        not the least.
      text: (Erasmus, Astoria)
    - is_correct: false
      rationale: Make sure to get the combination with the most rides, not the second
        most rides.
      text: (Upper East Side North, Upper East Side South)
    - is_correct: false
      rationale: Make sure to aggregate by the right column.
      text: (Midtown Center, Upper East Side North)
    question: Using the `zones_df` combined with the `march_yellow_rides_df`, which
      `pu_zone` `do_zone` pair had the most rides?
    question_name: Most Common Pickup-Dropoff Zone Pair
    question_reference_note: To answer this question, see the question "Most Common Pickup-Dropoff
      Zone Pair" in the notebook "06. Data Manipulation IV - Combining Data - Quiz.ipynb".
    reference_video: 6.1 Joining DataFrames with .join()
  - answers:
    - is_correct: false
      rationale: You might be measuring average `fare_amount` instead of average `passenger_count`...
      text: '13.526644'
    - is_correct: false
      rationale: You might be measuring average `trip_distance` instead of average
        `passenger_count`...
      text: '1.932876'
    - is_correct: true
      rationale: Exactly! Simply join `zones_df` into `rides_df` twice (once for pickup
        and once for dropoff), filter for the right pickup and dropoff zone, and then
        take the `.mean()` `passenger_count`!
      text: '1.277752'
    - is_correct: false
      rationale: You might be measuring average `total_amount` instead of average
        `passenger_count`...
      text: '21.211721'
    question: What is the average `passenger_count` for rides that started in the
      zone "Midtown Center" and ended in the zone "Upper East Side North"?
    question_name: Average Passenger Count for Specific Route
    question_reference_note: To answer this question, see the question "Average Passenger
      Count for Specific Route" in the notebook "06. Data Manipulation IV - Combining
      Data - Quiz.ipynb".
    reference_video: 6.1 Joining DataFrames with .join()
  - answers:
    - is_correct: false
      rationale: Make sure you're not concatenating toy_1_df to itself!
      text: (6, 2)
    - is_correct: true
      rationale: Since `toy_2_df` has a column that `toy_1_df` doesn't, an extra column
        is added.
      text: (7, 3)
    - is_correct: false
      rationale: Make sure you're not concatenating toy_2_df to itself!
      text: (8, 3)
    - is_correct: false
      rationale: This would be correct if toy_2_df didn't have the extra column "c"...
      text: (7, 2)
    question: Take the two toy dataframes below and concatenate them diagonally. What
      is the shape of the result?
    question_name: Diagonal Concatenation Result Shape
    question_reference_note: To answer this question, see the question "Diagonal Concatenation
      Result Shape" in the notebook "06. Data Manipulation IV - Combining Data - Quiz.ipynb".
    reference_video: 6.1 Concatenating DataFrames with .concat()
- module_name: 07. Data Manipulation V - Working With Data Types
  quiz_questions:
  - answers:
    - is_correct: false
      rationale: You might be sorting ascending rather than descending.
      text: day=6, hour=2
    - is_correct: true
      rationale: When sorting with `descending=True`, this is indeed the top result!
      text: day=3, hour=4
    - is_correct: false
      rationale: You might be taking the average `total_amount` rather than the average
        `fare_amount`.
      text: day=7, hour=23
    - is_correct: false
      rationale: You might be taking the weekday and hour from `tpep_dropoff_datetime`
        instead of `tpep_pickup_datetime`.
      text: day=1, hour=0
    question: Extract the day of the week (as a string) and the hour from the 'tpep_pickup_datetime'
      column. Then, calculate the average fare amount for each day-hour combination,
      and sort the results by average fare amount. Which day-hour combination had
      the highest average fare amount?
    question_name: Highest Average Fare by Day and Hour
    question_reference_note: To answer this question, see the question "Highest Average
      Fare by Day and Hour" in the notebook "07. Data Manipulation V - Working With
      Data Types - Quiz.ipynb".
    reference_video: 7.4 Working with Temporal Columns - the .dt Namespace
  - answers:
    - is_correct: true
      rationale: Nice! You could solve this in a few ways; a most straightforward
        way is to simply create the `trip_duration` column and sort descending.
      text: Saint Michaels Cemetery/Woodside
    - is_correct: false
      rationale: You might be taking the `do_zone` with the lowest trip duration.
      text: Midtown Center
    - is_correct: false
      rationale: Almost right! But the answer should be a `do_zone`, not a `do_location_id`.
      text: '207'
    - is_correct: false
      rationale: Make sure to use `.total_seconds()`!
      text: Woodside
    question: Which is the `do_zone` with the highest trip duration (where "trip duration"
      is measured as the `.total_seconds()` between `tpep_pickup_datetime` and `tpep_dropoff_datetime`)?
    question_name: Dropoff Zone with Longest Trip Duration
    question_reference_note: To answer this question, see the question "Dropoff Zone with
      Longest Trip Duration" in the notebook "07. Data Manipulation V - Working With
      Data Types - Quiz.ipynb".
    reference_video: 7.4 Working with Temporal Columns - the .dt Namespace
  - answers:
    - is_correct: true
      rationale: Exactly! Note that aggregating all elements of the group like this
        into a list simply takes _all_ values (i.e. it's not as if it takes unique
        values). As such, we could have even computed the same result by simply doing
        `.agg(pl.len())`!
      text: '161'
    - is_correct: false
      rationale: Are you sure you didn't find the `pu_location_id` with the shortest
        list of associated `do_zone`s?
      text: '5'
    - is_correct: false
      rationale: We are looking for the `pu_location_id` with the longest list of
        associated `do_zone`s, not the `pu_zone`!
      text: Midtown Center
    - is_correct: false
      rationale: It looks like you chose the wrong sort order, and looked for `pu_zone`
        rather than `pu_location_id`!
      text: Arden Heights
    question: 'With a group-by in `polars`, instead of finding some aggregate summary
      statistic for each group, you can also collect all the elements for each group
      into a list by simply passing in the column you''d like to aggregate to a list
      as a name (see below). With this, for each `pu_location_id`, make a column that
      aggregates all the `do_zones` associated with that `pu_location_id`; what is
      the `pu_location_id` with the longest list of associated `do_zone`s (hint: use
      the `.list` namespace)?'
    question_name: Pickup Location with Most Diverse Dropoff Zones
    question_reference_note: To answer this question, see the question "Pickup Location
      with Most Diverse Dropoff Zones" in the notebook "07. Data Manipulation V -
      Working With Data Types - Quiz.ipynb".
    reference_video: 7.3 Working with List Columns - the .list Namespace
  - answers:
    - is_correct: true
      rationale: Correct! This is indeed a common word used across many neighborhoods
        of New York City.
      text: Park
    - is_correct: false
      rationale: The question specifies "excluding null"... take another look!
      text: '`null`'
    - is_correct: false
      rationale: Make sure to take the 2nd element, not the 0th!
      text: East
    - is_correct: false
      rationale: Make sure to take the 2nd element, not the 3rd!
      text: North
    question: Using just `zones_df`, split `zone` into a list on `" "` as seen during
      the module, and take the 2nd element of every list using `.list.get()`. Then,
      using `group_by`, answer the question--what is the most commonly occurring second
      word in `zones_df` (excluding `null`)?
    question_name: Most Common Second Word in Zone Names
    question_reference_note: To answer this question, see the question "Most Common Second
      Word in Zone Names" in the notebook "07. Data Manipulation V - Working With
      Data Types - Quiz.ipynb".
    reference_video: 7.3 Working with List Columns - the .list Namespace
  - answers:
    - is_correct: false
      rationale: Make sure to use "less than", not "less than or equal to".
      text: '28426'
    - is_correct: false
      rationale: Make sure to use "greater than", not "greater than or equal to".
      text: '27994'
    - is_correct: true
      rationale: Perfect! With what we've learned so far, you could solve this with
        an `.and()` combination of `.gt()` and `.lt()`; however, for a more advanced
        technique for answering this question, on your own time you can check out
        the function `.is_between()`.
      text: '27538'
    - is_correct: false
      rationale: Check again, you might have got the order of the pickup-dropoff subtraction
        incorrect!
      text: '1'
    question: How many rides had a duration of more than 60 seconds and less than
      120 seconds?
    question_name: Rides with Duration Between 60-120 Seconds
    question_reference_note: To answer this question, see the question "Rides with Duration
      Between 60-120 Seconds" in the notebook "07. Data Manipulation V - Working With
      Data Types - Quiz.ipynb".
    reference_video: 7.4 Working with Temporal Columns - the .dt Namespace
  - answers:
    - is_correct: false
      rationale: Are you sure you're not accidentally using `.total_minutes()`?
      text: '3541969'
    - is_correct: true
      rationale: Exactly! You could have of course still used `.dt.total_seconds()`
        if you'd really wanted to, but then you'd have to divide by the number of
        seconds in a day.
      text: '20'
    - is_correct: false
      rationale: Are you sure you're not accidentally using `.total_seconds()`?
      text: '3581500'
    - is_correct: false
      rationale: Make sure to use "greater than" and not "great than or equal to".
      text: '6'
    question: 'How many rides had a duration of more than 1 day (hint: instead of
      using `.dt.total_seconds()`, you can use `.dt.total_days()`)?'
    question_name: Rides Longer Than One Day
    question_reference_note: To answer this question, see the question "Rides Longer Than
      One Day" in the notebook "07. Data Manipulation V - Working With Data Types
      - Quiz.ipynb".
    reference_video: 7.4 Working with Temporal Columns - the .dt Namespace
  - answers:
    - is_correct: true
      rationale: Exactly! Indeed many neighborhoods in New York City have the word
        'North' in the name.
      text: '15'
    - is_correct: false
      rationale: Note that string containment checks are case-sensitive in polars,
        so checking for "north" won't work!
      text: '0'
    - is_correct: false
      rationale: Looks like you accidentally got the total number of rows! Perhaps
        you accidentally used `count()` or `len()`...
      text: '265'
    - is_correct: false
      rationale: Make sure to not accidentally check for "South"!
      text: '19'
    question: How many zones in `zones_df` contain the word "North"?
    question_name: Zones Containing 'North'
    question_reference_note: To answer this question, see the question "Zones Containing
      'North'" in the notebook "07. Data Manipulation V - Working With Data Types
      - Quiz.ipynb".
    reference_video: 7.2 Working with String Columns - the .str Namespace
  - answers:
    - is_correct: false
      rationale: Don't forget to reverse the string!
      text: East
    - is_correct: false
      rationale: Not quite, this is the second most common 0th word!
      text: tseW
    - is_correct: false
      rationale: You might be taking the 1st element of the list rather than the 0th!
      text: kraP
    - is_correct: true
      rationale: Exactly! `East` is a very common word in New York City neighborhood
        names, and `tsaE` is `East` spelled in reverse!
      text: tsaE
    question: 'For `zones_df`, what is the most common `0th` word in the `zone` column,
      spelled in reverse? (Hint: split the `zone` column by ` ` into a list of strings;
      then, take the `0th` element of each list in that column, and apply `.str.reverse()`
      to it.)'
    question_name: Most Common First Word in Zone Names (Reversed)
    question_reference_note: To answer this question, see the question "Most Common First
      Word in Zone Names (Reversed)" in the notebook "07. Data Manipulation V - Working
      With Data Types - Quiz.ipynb".
    reference_video: 7.2 Working with String Columns - the .str Namespace
- module_name: 08. Data Manipulation VI - Interoperation and IO
  quiz_questions:
  - answers:
    - is_correct: false
      rationale: Are you sure you're using the "col" orientation?
      text: (3, 5)
    - is_correct: true
      rationale: Exactly! With column orientation, each list in the list of lists
        gets loaded as a column of the dataframe.
      text: (5, 3)
    question: Given the following data in the form of a list of lists, create a dataframe
      using `pl.from_records()` with a column orientation. What is the shape of that
      dataframe?
    question_name: DataFrame Shape from List of Lists (Column Orientation)
    question_reference_note: To answer this question, see the question "DataFrame Shape
      from List of Lists (Column Orientation)" in the notebook "08. Data Manipulation
      VI - Interoperation and IO - Quiz.ipynb".
    reference_video: 8.1 Interoperating DataFrames with Native Python Objects
  - answers:
    - is_correct: true
      rationale: Exactly! With row orientation, each list in the list of lists gets
        loaded as a row of the dataframe.
      text: (3, 5)
    - is_correct: false
      rationale: Are you sure you're using the "row" orientation?
      text: (5, 3)
    question: Given the following data in the form of a list of lists, create a dataframe
      using `pl.from_records()` with a rows orientation. What is the shape of that
      dataframe?
    question_name: DataFrame Shape from List of Lists (Row Orientation)
    question_reference_note: To answer this question, see the question "DataFrame Shape
      from List of Lists (Row Orientation)" in the notebook "08. Data Manipulation
      VI - Interoperation and IO - Quiz.ipynb".
    reference_video: 8.1 Interoperating DataFrames with Native Python Objects
  - answers:
    - is_correct: true
      rationale: Exactly! Csv's don't store data types, so polars has to do type-inference
        upon loading data. It does so liberally, usually inferring any integer as
        an `i64`.
      text: 'Yes'
    - is_correct: false
      rationale: Are you sure? Make sure you're using any extra input arguments to
        `.read_csv()`, and let polars infer the datatypes.
      text: 'No'
    question: Save out the following dataframe to a csv with `.write_csv()` to a file
      called `"./temp_file.csv"`. Then, read it back in with `.read_csv()`. Have the
      datatypes changed?
    question_name: Datatype Changes After CSV Write and Read
    question_reference_note: To answer this question, see the question "Datatype Changes
      After CSV Write and Read" in the notebook "08. Data Manipulation VI - Interoperation
      and IO - Quiz.ipynb".
    reference_video: 8.5 DataFrame IO
  - answers:
    - is_correct: true
      rationale: Exactly! ndjson files don't store data types, so polars has to do
        type-inference upon loading data. It does so liberally, usually inferring
        any integer as an `i64`.
      text: 'Yes'
    - is_correct: false
      rationale: Are you sure? Make sure you're using any extra input arguments to
        `.read_ndjson()`, and let polars infer the datatypes.
      text: 'No'
    question: Save out the following dataframe to a ndjson file with `.write_ndjson()`
      to a file called `"./temp_file.njson"`. Then, read it back in with `.read_ndjson()`.
      Have the datatypes changed?
    question_name: Datatype Changes After NDJSON Write and Read
    question_reference_note: To answer this question, see the question "Datatype Changes
      After NDJSON Write and Read" in the notebook "08. Data Manipulation VI - Interoperation
      and IO - Quiz.ipynb".
    reference_video: 8.5 DataFrame IO
  - answers:
    - is_correct: false
      rationale: Are you sure? Make sure you're not transforming the dataframe in
        any way before saving it with `.write_parquet()`.
      text: 'Yes'
    - is_correct: true
      rationale: Exactly! One of the nice things about `parquet` is that it stores
        schema along with the data, so the data gets loaded back in the way it was
        before it was saved, without any extra work!
      text: 'No'
    question: Save out the following dataframe to a parquet file with `.write_parquet()`
      to a file called `"./temp_file.parquet"`. Then, read it back in with `.read_parquet()`.
      Have the datatypes changed?
    question_name: Datatype Changes After Parquet Write and Read
    question_reference_note: To answer this question, see the question "Datatype Changes
      After Parquet Write and Read" in the notebook "08. Data Manipulation VI - Interoperation
      and IO - Quiz.ipynb".
    reference_video: 8.5 DataFrame IO
- module_name: 09. Integrating Polars Into the Data Science Workflow
  quiz_questions:
  - answers:
    - is_correct: false
      rationale: We are not including self-correlation, here.
      text: '`passenger_count`'
    - is_correct: false
      rationale: We are looking for the lowest absolute correlation, so don't forget
        to take the absolute value!
      text: '`extra`'
    - is_correct: true
      rationale: Exactly! By using `.corr()`, we can compute correlations, and `passenger_count`'s
        most highly correlated feature is `trip_distance`!
      text: '`trip_distance`'
    - is_correct: false
      rationale: We are looking for the least correlated, not the most correlated!
      text: '`vendor_id`'
    question: 'Using `rides_df_raw`, which feature is least correlated with `passenger_count`
      (either negatively or positively)? (Hint: you might need the polars function
      for absolute value, `.abs()`. Also, please filter out `null` values as done
      in the module!)'
    question_name: Feature Least Correlated with Passenger Count
    question_reference_note: To answer this question, see the question "Feature Least Correlated
      with Passenger Count" in the notebook "09. Integrating Polars Into the Data
      Science Workflow - Quiz.ipynb".
    reference_video: 9.2 Brief Data Exploration - Plots, Correlations, and Summary
      Statistics with Polars
  - answers:
    - is_correct: false
      rationale: There is no such correlation line.
      text: There is a second sub-majority of the data which adheres to a correlation
        line which has a slope of approximately `$20/mile - $22/mile`.
    - is_correct: true
      rationale: This statement is true; there is some data which deviates from this
        trend, but it is the majority.
      text: The majority of the data adheres to a correlation line which has a slope
        of approximately `$5/mile - $7/mile`.
    - is_correct: true
      rationale: This statement is true, in fact there is a highly non-neglible amount
        of rides with a negative trip distance.
      text: Some rides appear to have a negative trip distance.
    - is_correct: true
      rationale: This statement is true, in fact there is a spike of data along the
        y-axis, where trip distance equals 0.
      text: A non-negligible minority of the data appears to have a trip distance
        of exactly 0.
    question: 'Plot `total_amount` as a function of `trip_distance`. Which of the
      following statements about the resultant plot are true (hint: there are three
      correct answers)?'
    question_name: Total Amount vs Trip Distance Plot Analysis
    question_reference_note: To answer this question, see the question "Total Amount vs
      Trip Distance Plot Analysis" in the notebook "09. Integrating Polars Into the
      Data Science Workflow - Quiz.ipynb".
    reference_video: 9.2 Brief Data Exploration - Plots, Correlations, and Summary
      Statistics with Polars
  - answers:
    - is_correct: true
      rationale: Exactly! The distribution looks like a unimodal log normal distribution!
      text: Unimodal
    - is_correct: false
      rationale: Are you sure? Make sure to exclude any spikes of noise in your reasoning.
      text: Multimodal
    question: 'Plot an ECDF of ''fare_amount''. Is the resultant distribution unimodal
      or multimodal (i.e. is there one peak to the distribution or multiple)? (Hint:
      exclude any noisy spikes!)'
    question_name: Fare Amount Distribution Analysis
    question_reference_note: To answer this question, see the question "Fare Amount Distribution
      Analysis" in the notebook "09. Integrating Polars Into the Data Science Workflow
      - Quiz.ipynb".
    reference_video: 9.2 Brief Data Exploration - Plots, Correlations, and Summary
      Statistics with Polars
  - answers:
    - is_correct: true
      rationale: Exactly! The data can be passed directly into `mean_absolute_error`
        as Polars series.
      text: 'True'
    - is_correct: false
      rationale: Are you sure? Make sure you're using the correct function from scikit-learn!
      text: 'False'
    question: 'Given the following toy dataframe of `y_predicted` and `y_truth`, measure
      the `mean_absolute_error`. True or False: the result is greater than `.5`. (Hint:
      use the `sklearn` implementation of `mean_absolute_error`.)'
    question_name: Mean Absolute Error Calculation
    question_reference_note: To answer this question, see the question "Mean Absolute Error
      Calculation" in the notebook "09. Integrating Polars Into the Data Science Workflow
      - Quiz.ipynb".
    reference_video: 9.5 Machine Learning Model Building, Evaluation, and Discussion
  - answers:
    - is_correct: false
      rationale: Looks like the `.sample()` didn't work--`3582628` is just the size
        of the entire dataframe!
      text: '3582628'
    - is_correct: false
      rationale: If you're going to pass in a fraction as an argument, you have to
        pass it to the keyword argument "fraction"!
      text: '2'
    - is_correct: false
      rationale: Make sure to convert `2%` to a fraction, and use the "fraction" keyword
        argument!
      text: '0'
    - is_correct: true
      rationale: Exactly! There are 3582628 rows in the original dataframe, and 2%
        of 3582628 is 71652.
      text: '71652'
    question: In the module, we reviewed the function `.sample()`, and used it to
      reduce our data to a fixed number of rows; to this end, we passed in simply
      the number of rows that we wanted in the result with e.g. `.sample(10000)`.
      However, `.sample()` also provides the option to pass in a fraction of rows,
      with `.sample(fraction=X)`, where `X` must be between 0 and 1. Use this new
      way of using the function to reduce the data to 2% of its original size. What
      is the shape of the result?
    question_name: Sampling DataFrame with Fraction
    question_reference_note: To answer this question, see the question "Sampling DataFrame
      with Fraction" in the notebook "09. Integrating Polars Into the Data Science
      Workflow - Quiz.ipynb".
    reference_video: 9.2 Brief Data Exploration - Plots, Correlations, and Summary
      Statistics with Polars
- module_name: 10. Summative
  quiz_questions:
  - answers:
    - is_correct: true
      rationale: Both `id` and `street_number` get loaded as `i64`.
      text: '`i64`'
    - is_correct: false
      rationale: Though both `street_number` and `id` appear to be strictly positive,
        Polars's default behavior is to load integers as `i64`.
      text: '`u64`'
    - is_correct: false
      rationale: Though both `street_number` and `id` appear to fit in the range of
        8-bit values, Polars's default behavior is to load integers as `i64`.
      text: '`i8`'
    - is_correct: true
      rationale: '`street` gets loaded as a `str`.'
      text: '`str`'
    question: Create a dataframe from the following data. Which of the following data
      types can be found in the resultant dataframe?
    question_name: Identifying Data Types in New DataFrame
    question_reference_note: To answer this question, see the question "Identifying Data
      Types in New DataFrame" in the notebook "10. Summative - Quiz.ipynb".
  - answers:
    - is_correct: false
      rationale: An error is indeed thrown, but it's something besides this.
      text: An error is thrown, stating that "`schema_overrides` only works on `str`
        columns".
    - is_correct: false
      rationale: The code isn't able to run successfully, take another look at your
        code!
      text: The code runs successfully, casting the would-be `float` column to `pl.Int64`
        upon instantiation of the dataframe.
    - is_correct: true
      rationale: '`float` data cannot be cast to `pl.Int64` upon reading data--it
        can certainly happen later though, once the data has been read!'
      text: An error is thrown, stating that data from the column can't be parsed
        to `pl.Int64`.
    - is_correct: false
      rationale: The code isn't able to run successfully, take another look at your
        code!
      text: The code runs successfully, ignoring the schema override and simply loading
        the data as `pl.Float64`
    question: Load the rides data from csv, using `schema_overrides` to force `trip_distance`
      to be `pl.Int64`. What happens?
    question_name: Schema Override for Trip Distance
    question_reference_note: To answer this question, see the question "Schema Override
      for Trip Distance" in the notebook "10. Summative - Quiz.ipynb".
  - answers:
    - is_correct: false
      rationale: This is the minimum `congestion_surcharge`; make sure you're checking
        for maximum!
      text: '-2.5'
    - is_correct: false
      rationale: This is the maximum `fare_amount`; make sure you're taking the right
        column!
      text: '900.0'
    - is_correct: true
      rationale: Exactly! You take this by using the `.max()` function.
      text: '2.5'
    - is_correct: false
      rationale: Are you sure you're using the correct column?
      text: '3.4'
    question: What is the maximum `congestion_surcharge` in `rides_df_raw`?
    question_name: Maximum Congestion Surcharge
    question_reference_note: To answer this question, see the question "Maximum Congestion
      Surcharge" in the notebook "10. Summative - Quiz.ipynb".
  - answers:
    - is_correct: false
      rationale: 'Make sure you''re sorting by the columns in precisely the correct
        order: congestion_surcharge, tip_amount, then trip_distance!'
      text: '176836.3'
    - is_correct: false
      rationale: Make sure to sort descending, not ascending!
      text: '0.0'
    - is_correct: false
      rationale: Are you sure you're selecting the correct column?
      text: '166.1'
    - is_correct: true
      rationale: Exactly! This is the top row after sorting by all the requisite columns.
      text: '28.9'
    question: 'Sort `rides_df_raw` descending in the following order: `congestion_surcharge`,
      `tip_amount`, `trip_distance`. What is the `trip_distance` of the top trip?'
    question_name: Top Trip Distance After Sorting
    question_reference_note: To answer this question, see the question "Top Trip Distance
      After Sorting" in the notebook "10. Summative - Quiz.ipynb".
  - answers:
    - is_correct: true
      rationale: Indeed, we arrive to this answer by using `.gt()` and `.lt()`, after converting to kilometers!
      text: '838278'
    - is_correct: false
      rationale: Don't forget to convert miles to kilometers!
      text: '1112153'
    - is_correct: false
      rationale: Make sure to take the number of rows for your answer, not the number
        of rows!
      text: '1'
    - is_correct: false
      rationale: Are you sure you're using the right column?
      text: '99283'
    question: How many trips had a trip_distance greater than 1km and less than 2km?
    question_name: Trips Within Distance Range
    question_reference_note: To answer this question, see the question "Trips Within Distance
      Range" in the notebook "10. Summative - Quiz.ipynb".
  - answers:
    - is_correct: true
      rationale: To compare these two data types, something must be done to make them
        comparable, either casting the integer to a string or casting the string to
        an integer somehow.
      text: The code crashes because a string cannot be compared with an integer.
    - is_correct: false
      rationale: There's no problem with doing this, and in fact we did it in the
        module!
      text: The code crashes a boolean column cannot be added to a dataframe with
        `.with_columns()`.
    - is_correct: false
      rationale: Polars does not do type cating like this--you must request it explicitly.
      text: The code runs successfully, adding a column that checks if the `do_zone`
        is alphabetically greater than the string "0".
    - is_correct: false
      rationale: Polars does not do this kind of handling, though you could likely
        write some behavior using column expressions to do something similar!
      text: The code runs successfully; however, since strings and integers can't
        be compared in Polars, the resultant column is null everywhere.
    question: Try to add a column code which checks if `do_zone` is greater than 0.
      What happens, and why?
    question_name: Comparison of Different Data Types
    question_reference_note: To answer this question, see the question "Comparison of Different
      Data Types" in the notebook "10. Summative - Quiz.ipynb".
  - answers:
    - is_correct: false
      rationale: This almost works, but it accidentally includes `vendor_id` in the
        renaming!
      text: '`.select(pl.col(pl.Int32).name.suffix("_renamed"))`'
    - is_correct: true
      rationale: Using the `.name.suffix()` function, we can accomplish the same as
        what's being done by `.rename()`.
      text: '`.select(pl.col(["do_location_id", "pu_location_id"]).name.suffix("_renamed"))`'
    - is_correct: false
      rationale: Using alias will simply replace the column name altogether, not add
        a suffix! Furthermore, this code will rename both columns to the same thing,
        which will give an error.
      text: '`.select(pl.col(["do_location_id", "pu_location_id"]).alias("_renamed"))`'
    - is_correct: false
      rationale: This will select the wrong columns!
      text: '`.select(pl.String).name.suffix("_renamed"))`'
    question: See the following renaming which uses `.select` and `.rename()`. What
      is the expression using `.select` and name transformations from the `.name`
      namespace that accomplishes the same function?
    question_name: Equivalent Renaming Expression
    question_reference_note: To answer this question, see the question "Equivalent Renaming
      Expression" in the notebook "10. Summative - Quiz.ipynb".
  - answers:
    - is_correct: false
      rationale: Make sure you're checking for `pl.Int8` columns, not `pl.Int32` columns.
      text: (3582628, 28)
    - is_correct: false
      rationale: Make sure to use `.with_columns()`, not `.select()`.
      text: (0, 0)
    - is_correct: true
      rationale: Exactly! There are no columns with this datatype, so the shape remains
        the same.
      text: (3582628, 25)
    - is_correct: false
      rationale: Make sure you're checking for `pl.Int8` columns, not `pl.Float64`
        columns.
      text: (3582628, 35)
    question: Add a few new columns to the dataframe that copy all `pl.Int8` columns,
      simply giving them the suffix `_new`. What is the size of the resultant dataframe?
    question_name: Adding Suffixed Columns for Int8 Type
    question_reference_note: To answer this question, see the question "Adding Suffixed
      Columns for Int8 Type" in the notebook "10. Summative - Quiz.ipynb".
  - answers:
    - is_correct: false
      rationale: This is the value of `congestion_surcharge` with the least occurrences
        in the dataset, try again!
      text: '-0.75'
    - is_correct: false
      rationale: Are you sure you're using the correct column?
      text: '202.18'
    - is_correct: false
      rationale: Are you sure you're using the correct column?
      text: '134.51'
    - is_correct: true
      rationale: Exactly! We can group by `congestion_surcharge`, aggregate for `.len()`, and sort.
      text: '2.5'
    question: Using `.group_by()`, what is the most common value for `congestion_surcharge`
      in the dataset?
    question_name: Most Common Congestion Surcharge
    question_reference_note: To answer this question, see the question "Most Common Congestion
      Surcharge" in the notebook "10. Summative - Quiz.ipynb".
  - answers:
    - is_correct: true
      rationale: Exactly right, we find this by viewing the appropriate cell in the pivot table.
      text: '104748'
    - is_correct: false
      rationale: Close! This is the number of rides with 2 passengers that did have
        a tip; make sure to choose the correct cell from the pivot table!
      text: '349149'
    - is_correct: false
      rationale: Your answer might look like this if you used "sum" instead of "len"
        as an aggregate_function. Try again!
      text: 1.7435e6
    - is_correct: false
      rationale: The `pivot_table` is easier to read when you first create a boolean
        colummn for "tip_amount is 0", as instructed in the question! Check again
        ;)
      text: '1358'
    question: Create a `.pivot_table()`, where the rows are if the `tip_amount` was
      0 and the columns are `passenger_count`; for each combination of "`tip_amount`
      is 0" and `passenger_count`, compute the number of instances by using `aggregate_function`
      `"len"`. How many rides had a `tip_amount` of 0 and a `passenger_count` of 2?
    question_name: Zero-Tip Two-Passenger Rides Count
    question_reference_note: To answer this question, see the question "Zero-Tip Two-Passenger
      Rides Count" in the notebook "10. Summative - Quiz.ipynb".
  - answers:
    - is_correct: false
      rationale: This is the result you'd get if you joined `name_df` into `restaurant_df`...
        make sure to get the order correct!
      text: (5, 3)
    - is_correct: false
      rationale: This would happen if you tried to join using the `on` input argument.
        Be sure to use `left_on` and `right_on` since the `restaurant_name` column
        has a different name in both dataframes!
      text: The code doesn't run, throwing a "ColumnNotFoundError".
    - is_correct: true
      rationale: Exactly! The join columns combine into one for a total of 3 columns.
      text: (4, 3)
    - is_correct: false
      rationale: This is the result you'd get if you were doing an anti-join... make
        sure to do a left-join!
      text: (0, 2)
    question: Join the following two dataframes using a left-join (`restaurant_df`
      into `name_df`, on the name of the restaurant). What is the shape of the resultant
      dataframe?
    question_name: Left-Join Result Shape
    question_reference_note: To answer this question, see the question "Left-Join Result
      Shape" in the notebook "10. Summative - Quiz.ipynb".
  - answers:
    - is_correct: false
      rationale: This is the result you'd get if you anti-joined `name_df` into `restaurant_df`...
        make sure to get the order correct!
      text: (1, 2)
    - is_correct: false
      rationale: This would happen if you tried to join using the `on` input argument.
        Be sure to use `left_on` and `right_on` since the `restaurant_name` column
        has a different name in both dataframes!
      text: The code doesn't run, throwing a "ColumnNotFoundError".
    - is_correct: false
      rationale: This is the result you'd get if you were doing a left-join... try
        again!
      text: (4, 3)
    - is_correct: true
      rationale: Exactly! There is nothing in `names_df` that is missing a row to
        join with in `restaurant_df`.
      text: (0, 2)
    question: Join the following two dataframes using an anti-join (`restaurant_df`
      into `name_df`, on the name of the restaurant). What is the shape of the resultant
      dataframe?
    question_name: Anti-Join Result Shape
    question_reference_note: To answer this question, see the question "Anti-Join Result
      Shape" in the notebook "10. Summative - Quiz.ipynb".
  - answers:
    - is_correct: true
      rationale: Exactly, there are a few nulls in the dataframe to begin with, and
        there are even more after a diagonal concatenation!
      text: '20'
    - is_correct: false
      rationale: This is how many there are across the dataframes to begin with! Make
        sure to do the concatenation before counting nulls, as the concatenation process
        might add some more...
      text: '6'
    - is_correct: false
      rationale: Some dataframes here have non-overlapping columns, so make sure to
        set "how" as "diagonal"!
      text: The code doesn't run due to a `ShapeError`
    - is_correct: false
      rationale: Are you replacing the null values with something else somehow?
      text: '0'
    question: Vertically concatenate the following dataframes. How many null values
      are there in the entire resultant dataframe?
    question_name: Null Count After Vertical Concatenation
    question_reference_note: To answer this question, see the question "Null Count After
      Vertical Concatenation" in the notebook "10. Summative - Quiz.ipynb".
  - answers:
    - is_correct: false
      rationale: Make sure to check rides before `2024-03-15`... you might be checking
        rides with `.le()` rather than `.lt()`.
      text: '0.455174'
    - is_correct: true
      rationale: '`2024-3-15` is halfway through the month, so it makes sense that
        the number here is almost `0.5`!'
      text: '0.455173'
    - is_correct: false
      rationale: 1.0 is all the rides! Double check your expression...
      text: '1.0'
    - is_correct: false
      rationale: We're looking for the fraction of rides that started before `2024-03-15`,
        not the number of rides!
      text: '1630716'
    question: What fraction of rides in the dataframe started before `2024-03-15`
      (i.e. had a `tpep_pickup_datetime` before `2024-03-15`?
    question_name: Fraction of Early March Rides
    question_reference_note: To answer this question, see the question "Fraction of Early
      March Rides" in the notebook "10. Summative - Quiz.ipynb".
  - answers:
    - is_correct: true
      rationale: If you want to concatenate the lists together, you'd have to take
        a different approach! This is for your advanced learning ;)
      text: The code runs smoothly, but the resultant aggregated columns is filled
        with just nulls.
    - is_correct: false
      rationale: This is the goal, but it'd required an advanced technique to achieve!
      text: The code runs smoothly, the resultant column being a combined list of
        all `salient_appendages` for that animal class.
    - is_correct: false
      rationale: You're correct that the code doesn't run successfully in achieving
        the desired effect, however it doesn't quite throw an error.
      text: The code crashes with a dataype error, saying that "list columns cannot
        be added together in a `group_by`".
    - is_correct: false
      rationale: This is what would happen if we didn't use the `.sum()` function
        and just left the agg as a simple `.agg(pl.col("salient_appendages"))`.
      text: The code runs smoothly, the resultant column being a list of lists of
        all values for `salient_appendages` for that animal class.
    question: Group the following dataframe by `class`, and `.sum()` the `salient_appendages`
      column (which is a list type) to create a list of all `salient_appendages` for
      that animal class. What happens?
    question_name: List Column Aggregation Behavior
    question_reference_note: To answer this question, see the question "List Column Aggregation
      Behavior" in the notebook "10. Summative - Quiz.ipynb".
  - answers:
    - is_correct: true
      rationale: Exactly! Weekend pickups have an average `tip_amount` of 2.97, while
        non-weekend pickups have an average `tip_amount` of 3.29.
      text: 'False'
    - is_correct: false
      rationale: Are you sure you created correctly the column for grouping? Try again!
      text: 'True'
    question: 'True or False: weekend taxi trips (trips that have a `tpep_pickup_datetime`
      on Saturday or Sunday) have on average higher tip amounts than non-weekend taxi
      trips.'
    question_name: Weekend vs Weekday Tip Comparison
    question_reference_note: To answer this question, see the question "Weekend vs Weekday
      Tip Comparison" in the notebook "10. Summative - Quiz.ipynb".
  - answers:
    - is_correct: true
      rationale: Exactly! The data were Float64's in the numpy array, and they stayed
        that way when they got converted to a Polars dataframe.
      text: '`f64`, `f64`'
    - is_correct: false
      rationale: Despite being random floats between just 0 and 1, the floats are
        still 64 bit precision.
      text: '`f32`, `f32`'
    - is_correct: false
      rationale: Is it possible that you transposed the data somehow?
      text: '`f64`, `f64`, `f64`, `f64`'
    - is_correct: false
      rationale: There is no problem creating a polars dataframe from a numpy array
        like this!
      text: The code doesn't run, due to a datatype conversion error
    question: Convert the following numpy array to a polars dataframe. What are the
      datatypes of the resultant dataframe?
    question_name: Numpy to Polars Dataframe Conversion
    question_reference_note: To answer this question, see the question "Numpy to Polars
      Dataframe Conversion" in the notebook "10. Summative - Quiz.ipynb".
  - answers:
    - is_correct: false
      rationale: This shouldn't be the case... perhaps you accidentally converted
        it to a dataframe yourself!
      text: 'True'
    - is_correct: true
      rationale: Exactly! Polars handles this type of interoperation without a problem,
        cleanly able to convert a Polars Series to a Pandas Series and back.
      text: 'False'
    question: 'Create a Polars series, convert it to Pandas, and then convert it back
      to Polars. True or False: the result upon returning to Polars is now a single-column
      dataframe.'
    question_name: Polars to Pandas to Polars Series Conversion
    question_reference_note: To answer this question, see the question "Polars to Pandas
      to Polars Series Conversion" in the notebook "10. Summative - Quiz.ipynb".
  - answers:
    - is_correct: false
      rationale: Please don't forget to exclude correlation with self!
      text: '`tip_amount`'
    - is_correct: true
      rationale: Indeed, `total_amount` is most highly correlated with `tip_amount`,
        which makes sense since `total_amount` is a sum which includes `tip_amount`!
      text: '`total_amount`'
    - is_correct: false
      rationale: This is the feature least correlated with `tip_amount`! Make sure
        you're filtering, sorting, and/or selecting in the right direction.
      text: '`congestion_surcharge`'
    - is_correct: false
      rationale: Are you sure you're filtering out nulls before checking correlation?
      text: '`passenger_count`'
    question: Using `rides_df_raw`, which feature is most highly correlated (either
      positively or negatively) with `tip_amount` (excluding `tip_amount` itself)?
      Also, please filter out `null` values as done in the module!
    question_name: Feature Most Correlated with Tip Amount
    question_reference_note: To answer this question, see the question "Feature Most Correlated
      with Tip Amount" in the notebook "10. Summative - Quiz.ipynb".
  - answers:
    - is_correct: false
      rationale: 2:00 is not a peak; in fact it is the hour with the smallest `average_total_amount`.
      text: There appears to be a peak in average total amount around 2:00.
    - is_correct: true
      rationale: There is indeed a peak at this time! Check again your x-axis.
      text: There appears to be a peak in average total amount around 5:00.
    - is_correct: true
      rationale: There is indeed a peak at this time! Check again your x-axis.
      text: There appears to be a peak in average total amount around 16:00.
    - is_correct: true
      rationale: There is indeed a peak at this time! Check again your x-axis.
      text: There appears to be a peak in average total amount around 23:00.
    question: 'With `rides_df_raw`, make a plot of ''hour of day of taxi ride'' vs
      ''average total_amount''. Which of the following statements is True (hint: there
      are multiple options)?'
    question_name: Hourly Average Total Amount Analysis
    question_reference_note: To answer this question, see the question "Hourly Average Total
      Amount Analysis" in the notebook "10. Summative - Quiz.ipynb".
